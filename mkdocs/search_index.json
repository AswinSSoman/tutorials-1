{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome!\n\n\nABRPI training materials\n\n\nAntibiotic Resistant Pathogens Initiative\n\n\nThis site contains tutorials for using the ABRPI\n\nMicrobial Genomics Virtual Lab\n (the mGVL) to perform bioinformatics\ntasks on bacterial \nomics\n data, either on the Unix command line or using\nthe \nGalaxy\n system. Tutorials on Genomics, PacBio assembly, Transcriptomics and Proteomics can be found under the tabs in the top panel.\n\n\nIf you wish to set up your own instance (version) of the mGVL, follow the instructions \nhere\n. Note: at the stage where you select options in the GVL Launcher window, go to \nShow advanced startup options\n and under \nFlavor\n select \nMicrobial GVL with Tutorial Indices\n. In this mGVL instance, you can use both Galaxy and command line tools. If you wish to use the command line tools on a different computer (e.g. your local computer), you would need to make sure the required tools are installed (e.g. Canu, Circlator, Pilon, SPAdes, etc.).\n\n\nFor more information about how to use the Microbial Genomics Virtual Lab and Galaxy, see \nhttp://www.genome.edu.au/", 
            "title": "Home"
        }, 
        {
            "location": "/#welcome", 
            "text": "", 
            "title": "Welcome!"
        }, 
        {
            "location": "/#abrpi-training-materials", 
            "text": "", 
            "title": "ABRPI training materials"
        }, 
        {
            "location": "/#antibiotic-resistant-pathogens-initiative", 
            "text": "This site contains tutorials for using the ABRPI Microbial Genomics Virtual Lab  (the mGVL) to perform bioinformatics\ntasks on bacterial  omics  data, either on the Unix command line or using\nthe  Galaxy  system. Tutorials on Genomics, PacBio assembly, Transcriptomics and Proteomics can be found under the tabs in the top panel.  If you wish to set up your own instance (version) of the mGVL, follow the instructions  here . Note: at the stage where you select options in the GVL Launcher window, go to  Show advanced startup options  and under  Flavor  select  Microbial GVL with Tutorial Indices . In this mGVL instance, you can use both Galaxy and command line tools. If you wish to use the command line tools on a different computer (e.g. your local computer), you would need to make sure the required tools are installed (e.g. Canu, Circlator, Pilon, SPAdes, etc.).  For more information about how to use the Microbial Genomics Virtual Lab and Galaxy, see  http://www.genome.edu.au/", 
            "title": "Antibiotic Resistant Pathogens Initiative"
        }, 
        {
            "location": "/modules/galaxy/", 
            "text": "Get Data into Galaxy\n\n\nKeywords: Galaxy, Microbial Genomics Virtual Lab\n\n\nGalaxy Background\n\n\nGalaxy is a web-based analysis and workflow platform designed for biologists to analyse their own data. It can be used to run a variety of bioinformatics tools. The selection of bioinformatics tools installed on the Galaxy instance we are using today caters for the analysis of bacterial genomics data sets.\n\n\nGalaxy is an open, web-based platform. Details about the project can be found \nhere\n.\n\n\nThe Galaxy interface is separated into three parts. The \nTools\n list on the left, the \nViewing\n panel in the middle and the analysis and data \nHistory\n on the right.\n\n\n\n\nRegister in Galaxy\n\n\nOpen a new tab or window on your web browser. Use Firefox or Chrome - please don\u2019t use Internet Explorer or Safari.\n\n\nIn the address bar, type in the address of your galaxy server. Alternatively, you can access galaxy via the dashboard of your mGVL.\n\n\n\n\nClick on \nUser\n button on the right.\n\n\n\n\n\n\nSelect: \nUser \n Register\n\n\nEnter your email, choose a password, and choose a user name.\n\n\nClick \nSubmit\n\n\n\n\nImport a history\n\n\n\n\nIn the menu options across the top, go to \nShared Data\n.\n\n\nClick on \nHistories\n.\n\n\n\n\n\n\n\n\nA list of published histories should appear. Click on the history that you want to use.\n\n\nClick on \nImport history\n.\n\n\n\n\nAn option will appear to re-name the history. We don\nt need to rename it, so click \nImport\n.\n\n\n\n\n\n\nThe history will now appear in your Current History pane, and the files are ready to use in Galaxy analyses.\n\n\n\n\n\n\nOther ways to import data into Galaxy\n\n\n\n\nUpload a file from your computer\n\n\nCopy a link to a Galaxy history\n\n\nFor sample training data files to use, see the \nnext section.", 
            "title": "Starting with Galaxy"
        }, 
        {
            "location": "/modules/galaxy/#get-data-into-galaxy", 
            "text": "Keywords: Galaxy, Microbial Genomics Virtual Lab", 
            "title": "Get Data into Galaxy"
        }, 
        {
            "location": "/modules/galaxy/#galaxy-background", 
            "text": "Galaxy is a web-based analysis and workflow platform designed for biologists to analyse their own data. It can be used to run a variety of bioinformatics tools. The selection of bioinformatics tools installed on the Galaxy instance we are using today caters for the analysis of bacterial genomics data sets.  Galaxy is an open, web-based platform. Details about the project can be found  here .  The Galaxy interface is separated into three parts. The  Tools  list on the left, the  Viewing  panel in the middle and the analysis and data  History  on the right.", 
            "title": "Galaxy Background"
        }, 
        {
            "location": "/modules/galaxy/#register-in-galaxy", 
            "text": "Open a new tab or window on your web browser. Use Firefox or Chrome - please don\u2019t use Internet Explorer or Safari.  In the address bar, type in the address of your galaxy server. Alternatively, you can access galaxy via the dashboard of your mGVL.   Click on  User  button on the right.    Select:  User   Register  Enter your email, choose a password, and choose a user name.  Click  Submit", 
            "title": "Register in Galaxy"
        }, 
        {
            "location": "/modules/galaxy/#import-a-history", 
            "text": "In the menu options across the top, go to  Shared Data .  Click on  Histories .     A list of published histories should appear. Click on the history that you want to use.  Click on  Import history .   An option will appear to re-name the history. We don t need to rename it, so click  Import .    The history will now appear in your Current History pane, and the files are ready to use in Galaxy analyses.", 
            "title": "Import a history"
        }, 
        {
            "location": "/modules/galaxy/#other-ways-to-import-data-into-galaxy", 
            "text": "Upload a file from your computer  Copy a link to a Galaxy history  For sample training data files to use, see the  next section.", 
            "title": "Other ways to import data into Galaxy"
        }, 
        {
            "location": "/modules/data-dna/", 
            "text": "Dataset\n\n\nThis page contains data for the tutorials. The tutorials will specify whether to import a whole history (URLs listed in the first section) or an individual file (URLs listed in the second section).\n\n\nGalaxy histories: URLs\n\n\n\n\nGalaxy history of input files\n\n\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/Galaxy_history_input_files.tar.gz\n\n\n\n\nGalaxy history: FastQC\n\n\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/FastQChistory.tar.gz\n\n\n\n\nGalaxy history: Spades\n\n\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/Spadeshistory.tar.gz\n\n\n\n\nGalaxy history: Prokka\n\n\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/Prokkahistory.tar.gz\n\n\n\n\nGalaxy history: Snippy\n\n\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/Snippyhistory.tar.gz\n\n\nTo get the saved tutorial history (a set of files) into Galaxy:\n\n\n\n\nCopy the link address.\n\n\nGo to your Galaxy instance. Make sure you are registered and logged in. Refresh the page.\n\n\nClick on the \nHistory\n cog \n\n\nSelect \nImport from File\n\n\n\n\n\n\n\n\nIn the box called \nArchived History URL\n, paste in the link address to the Galaxy history.\n\n\nClick \nSubmit\n\n\nWait a few seconds.\n\n\nClick on the \nview all histories\n button \n\n\nSee if the Galaxy history has been imported: it will be called \nimported from archive: Data\n\n\nAbove that pane, click on the \nSwitch to\n button.\n\n\nThen click \nDone\n (in the top left corner).\n\n\nYou should now have a list of five files in your current history.\n\n\n\n\n\n\nIndividual input files\n\n\nWildtype reference\n\n\n\n\nwildtype.fna\n\n\n\n\n https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/wildtype.fna\n\n\n\n\nwildtype.gbk\n\n\n\n\n https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/wildtype.gbk\n\n\n\n\nwildtype.gff\n\n\n\n\n https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/wildtype.gff\n\n\nMutant Illumina sequence\n\n\n\n\nmutant_R1.fastq.gz\n\n\n\n\n https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/mutant_R1.fastq.gz\n\n\n\n\nmutant_R2.fastq.gz\n\n\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/mutant_R2.fastq.gz\n\n\nAssembled contigs\n\n\n\n\nSPAdes_contigs.fasta\n\n\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/SPAdes_contigs.fasta\n\n\nUpload to Galaxy\n\n\nThere are two ways to upload these files to Galaxy. You can either download to your local computer and upload to Galaxy, or you can tell Galaxy to directly upload the file from an external source.\n\n\nDownload and upload:\n\n\n\n\nDownload required file(s) to your computer.\n\n\nFrom the Galaxy tool panel, click on \nGet Data \n Upload File\n  \n\n\nClick the \nChoose local file\n button  \n\n\nFind and select the \nfile\n you downloaded and click \nOpen\n  \n\n\nSet the \nType\n correctly.  \n\n\nClick the \nStart\n button.  \n\n\nOnce the progress bar reaches 100%, click the \nClose\n button  \n\n\nThe file will now upload to your current history.\n\n\n\n\nOr, tell Galaxy to find the file from an external source:\n\n\n\n\nFrom the Galaxy tool panel, click on \nGet Data \n Upload File\n  \n\n\nClick the \nPaste/Fetch data\n button  \n\n\nPaste the URL into the box.\n\n\nClick the \nStart\n button.  \n\n\nOnce the progress bar reaches 100%, click the \nClose\n button  \n\n\nThe file will now upload to your current history.", 
            "title": "Training dataset"
        }, 
        {
            "location": "/modules/data-dna/#dataset", 
            "text": "This page contains data for the tutorials. The tutorials will specify whether to import a whole history (URLs listed in the first section) or an individual file (URLs listed in the second section).", 
            "title": "Dataset"
        }, 
        {
            "location": "/modules/data-dna/#galaxy-histories-urls", 
            "text": "Galaxy history of input files   https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/Galaxy_history_input_files.tar.gz   Galaxy history: FastQC   https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/FastQChistory.tar.gz   Galaxy history: Spades   https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/Spadeshistory.tar.gz   Galaxy history: Prokka   https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/Prokkahistory.tar.gz   Galaxy history: Snippy   https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/Snippyhistory.tar.gz  To get the saved tutorial history (a set of files) into Galaxy:   Copy the link address.  Go to your Galaxy instance. Make sure you are registered and logged in. Refresh the page.  Click on the  History  cog   Select  Import from File     In the box called  Archived History URL , paste in the link address to the Galaxy history.  Click  Submit  Wait a few seconds.  Click on the  view all histories  button   See if the Galaxy history has been imported: it will be called  imported from archive: Data  Above that pane, click on the  Switch to  button.  Then click  Done  (in the top left corner).  You should now have a list of five files in your current history.", 
            "title": "Galaxy histories: URLs"
        }, 
        {
            "location": "/modules/data-dna/#individual-input-files", 
            "text": "", 
            "title": "Individual input files"
        }, 
        {
            "location": "/modules/data-dna/#wildtype-reference", 
            "text": "wildtype.fna    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/wildtype.fna   wildtype.gbk    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/wildtype.gbk   wildtype.gff    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/wildtype.gff", 
            "title": "Wildtype reference"
        }, 
        {
            "location": "/modules/data-dna/#mutant-illumina-sequence", 
            "text": "mutant_R1.fastq.gz    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/mutant_R1.fastq.gz   mutant_R2.fastq.gz   https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/mutant_R2.fastq.gz", 
            "title": "Mutant Illumina sequence"
        }, 
        {
            "location": "/modules/data-dna/#assembled-contigs", 
            "text": "SPAdes_contigs.fasta   https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/SPAdes_contigs.fasta", 
            "title": "Assembled contigs"
        }, 
        {
            "location": "/modules/data-dna/#upload-to-galaxy", 
            "text": "There are two ways to upload these files to Galaxy. You can either download to your local computer and upload to Galaxy, or you can tell Galaxy to directly upload the file from an external source.  Download and upload:   Download required file(s) to your computer.  From the Galaxy tool panel, click on  Get Data   Upload File     Click the  Choose local file  button    Find and select the  file  you downloaded and click  Open     Set the  Type  correctly.    Click the  Start  button.    Once the progress bar reaches 100%, click the  Close  button    The file will now upload to your current history.   Or, tell Galaxy to find the file from an external source:   From the Galaxy tool panel, click on  Get Data   Upload File     Click the  Paste/Fetch data  button    Paste the URL into the box.  Click the  Start  button.    Once the progress bar reaches 100%, click the  Close  button    The file will now upload to your current history.", 
            "title": "Upload to Galaxy"
        }, 
        {
            "location": "/modules/velvet/", 
            "text": "Assembly using Velvet\n\n\nKeywords: de novo assembly, Velvet, Galaxy, Microbial Genomics Virtual Lab\n\n\nBackground\n\n\nVelvet is one of a number of \nde novo\n assemblers that use short read sets as input (e.g. Illumina Reads), and the assembly method is based on de Bruijn graphs. For information about Velvet see this \nlink\n.\n\n\n\n\n\nIn this activity, we will perform a \nde novo\n assembly of a short read set using the Velvet assembler.\n\n\nLearning objectives\n\n\nAt the end of this tutorial you should be able to:\n\n\n\n\nassemble the reads using Velvet, and\n\n\nexamine the output assembly.\n\n\n\n\nImport and view data\n\n\n\n\n\nSee \nhere\n for information about how to start with Galaxy, and \nhere\n for the link to import the \nGalaxy history of input files\n for this tutorial, if you don\nt already have them in your history.\n\n\n\n\nThe read set for today is from an imaginary \nStaphylococcus aureus\n bacterium with a miniature genome.\n\n\n\n\nThe whole genome shotgun method used to sequence our mutant strain read set was produced on an Illumina DNA sequencing instrument.\n\n\n\n\n\n\nThe files we need for assembly are the \nmutant_R1.fastq\n and \nmutant_R2.fastq\n.\n\n\n\n\n\n\n(We don\nt need the reference genome sequences for this tutorial).\n\n\n\n\n\n\nThe reads are paired-end.\n\n\n\n\n\n\nEach read is 150 bases long. \n\n\n\n\n\n\nThe number of bases sequenced is equivalent to 19x the genome sequence of the wildtype strain. (Read coverage 19x - rather low!).\n\n\n\n\n\n\n\n\n\n\n\nClick on the View Data button (the \n) next to one of the FASTQ sequence files.\n\n\n\n\n\n\n\nAssemble reads with Velvet\n\n\n\n\nWe will perform a \nde novo\n assembly of the mutant FASTQ reads into long contiguous sequences (in FASTA format.)\n\n\nVelvet requires the user to input a value of \nk\n for the assembly process. K-mers are fragments of sequence reads. Small k-mers will give greater connectivity, but large k-mers will give better specificity.\n\n\n\n\n\n\n\n\n\nGo to \nTools \n NGS Analysis \n NGS: Assembly \n velvet\n\n\n\n\nSet the following parameters (leave other settings as they are):\n\n\n\n\nK-mer\n: choose a value for k between 21 and 95\n\n\nInput file type\n: Fastq\n\n\nSingle or paired end reads\n: Paired\n\n\n Select first set of reads\n: \nmutant_R1.fastq\n  \n\n\n Select second set of reads\n: \nmutant_R2.fastq\n\n\n\n\n\n\n\n\nYour tool interface should look like this:\n\n\n\n\n\n\n\n\n\n\nClick \nExecute\n\n\n\n\nExamine the output\n\n\n\n\nGalaxy is now running velvet on the reads for you.\n\n\nPress the refresh button in the history pane to see if it has finished.\n\n\n\n\nWhen it is finished, you will have three new files in your history.  \n\n\n\n\na \nContigs\n file\n\n\na \nContigs stats\n file\n\n\nthe velvet \nlog\n file\n\n\n\n\n\n\n\n\nClick on the View Data button \n on each of the files.\n\n\n\n\n\n\nThe \nContigs\n file will show each contig with the \nk-mer length\n and \nk-mer coverage\n listed as part of the header (however, these are just called \nlength\n and \ncoverage\n).\n\n\n\n\nK-mer length\n: For the value of k chosen in the assembly, a measure of how many k-mers overlap (by 1 bp each overlap) to give this length.\n\n\nK-mer coverage\n: For the value of k chosen in the assembly, a measure of how many k-mers overlap each base position (in the assembly).\n\n\n\n\n\n\n\n\n\n\n\n\nThe \nContigs stats\n file will show a list of these k-mer lengths and k-mer coverages.\n\n\n\n\n\n\n\n\nWe will summarise the information in the \nlog\n file.\n\n\nGo to \nBasic Tools \n NGS Common Toolsets \n FASTA manipulation \n Fasta statistics\n\n\nFor the required input file, choose the velvet \nContigs\n file.\n\n\nClick \nExecute\n.\n\n\nA new file will appear called \nFasta summary stats\n\n\nClick the eye icon to look at this file.\n\n\n\n\n\n\n\n\nLook at:\n\n\nnum_seq\n: the number of contigs in the FASTA file.\n\n\nnum_bp\n: the number of assembled bases. Roughly proportional to genome size.\n\n\nlen_max\n: the biggest contig.  \n\n\nlen_N50\n: N50 is a contig size. If contigs were ordered from small to large, half of all the nucleotides will be in contigs this size or larger.", 
            "title": "Genome assembly with Velvet"
        }, 
        {
            "location": "/modules/velvet/#assembly-using-velvet", 
            "text": "Keywords: de novo assembly, Velvet, Galaxy, Microbial Genomics Virtual Lab", 
            "title": "Assembly using Velvet"
        }, 
        {
            "location": "/modules/velvet/#background", 
            "text": "Velvet is one of a number of  de novo  assemblers that use short read sets as input (e.g. Illumina Reads), and the assembly method is based on de Bruijn graphs. For information about Velvet see this  link .   In this activity, we will perform a  de novo  assembly of a short read set using the Velvet assembler.", 
            "title": "Background"
        }, 
        {
            "location": "/modules/velvet/#learning-objectives", 
            "text": "At the end of this tutorial you should be able to:   assemble the reads using Velvet, and  examine the output assembly.", 
            "title": "Learning objectives"
        }, 
        {
            "location": "/modules/velvet/#import-and-view-data", 
            "text": "See  here  for information about how to start with Galaxy, and  here  for the link to import the  Galaxy history of input files  for this tutorial, if you don t already have them in your history.   The read set for today is from an imaginary  Staphylococcus aureus  bacterium with a miniature genome.   The whole genome shotgun method used to sequence our mutant strain read set was produced on an Illumina DNA sequencing instrument.    The files we need for assembly are the  mutant_R1.fastq  and  mutant_R2.fastq .    (We don t need the reference genome sequences for this tutorial).    The reads are paired-end.    Each read is 150 bases long.     The number of bases sequenced is equivalent to 19x the genome sequence of the wildtype strain. (Read coverage 19x - rather low!).      Click on the View Data button (the  ) next to one of the FASTQ sequence files.", 
            "title": "Import and view data"
        }, 
        {
            "location": "/modules/velvet/#assemble-reads-with-velvet", 
            "text": "We will perform a  de novo  assembly of the mutant FASTQ reads into long contiguous sequences (in FASTA format.)  Velvet requires the user to input a value of  k  for the assembly process. K-mers are fragments of sequence reads. Small k-mers will give greater connectivity, but large k-mers will give better specificity.     Go to  Tools   NGS Analysis   NGS: Assembly   velvet   Set the following parameters (leave other settings as they are):   K-mer : choose a value for k between 21 and 95  Input file type : Fastq  Single or paired end reads : Paired   Select first set of reads :  mutant_R1.fastq      Select second set of reads :  mutant_R2.fastq     Your tool interface should look like this:      Click  Execute", 
            "title": "Assemble reads with Velvet"
        }, 
        {
            "location": "/modules/velvet/#examine-the-output", 
            "text": "Galaxy is now running velvet on the reads for you.  Press the refresh button in the history pane to see if it has finished.   When it is finished, you will have three new files in your history.     a  Contigs  file  a  Contigs stats  file  the velvet  log  file     Click on the View Data button   on each of the files.    The  Contigs  file will show each contig with the  k-mer length  and  k-mer coverage  listed as part of the header (however, these are just called  length  and  coverage ).   K-mer length : For the value of k chosen in the assembly, a measure of how many k-mers overlap (by 1 bp each overlap) to give this length.  K-mer coverage : For the value of k chosen in the assembly, a measure of how many k-mers overlap each base position (in the assembly).       The  Contigs stats  file will show a list of these k-mer lengths and k-mer coverages.     We will summarise the information in the  log  file.  Go to  Basic Tools   NGS Common Toolsets   FASTA manipulation   Fasta statistics  For the required input file, choose the velvet  Contigs  file.  Click  Execute .  A new file will appear called  Fasta summary stats  Click the eye icon to look at this file.     Look at:  num_seq : the number of contigs in the FASTA file.  num_bp : the number of assembled bases. Roughly proportional to genome size.  len_max : the biggest contig.    len_N50 : N50 is a contig size. If contigs were ordered from small to large, half of all the nucleotides will be in contigs this size or larger.", 
            "title": "Examine the output"
        }, 
        {
            "location": "/modules/spades/", 
            "text": "Assembly using Spades\n\n\nKeywords: de novo assembly, Spades, Galaxy, Microbial Genomics Virtual Lab\n\n\nBackground\n\n\nSpades is one of a number of \nde novo\n assemblers that use short read sets as input (e.g. Illumina Reads), and the assembly method is based on de Bruijn graphs. For information about Spades see this \nlink\n.\n\n\n\n\n\nIn this activity, we will perform a \nde novo\n assembly of a short read set using the Spades assembler. The output from Spades that we are interested in is a multiFASTA file that contains the draft genome sequence.\n\n\n\n\n\nLearning objectives\n\n\nAt the end of this tutorial you should be able to:\n\n\n\n\n\n\n\nassemble the reads using Spades, and\n\n\nexamine the output assembly.\n\n\n\n\n\n\n\nImport and view data\n\n\n\n\n\nSee \nhere\n for information about how to start with Galaxy, and \nhere\n for the link to import the \nGalaxy history of input files\n for this tutorial, if you don\nt already have them in your history.\n\n\n\n\nThe read set for today is from an imaginary \nStaphylococcus aureus\n bacterium with a miniature genome.\n\n\n\n\nThe whole genome shotgun method used to sequence our mutant strain read set was produced on an Illumina DNA sequencing instrument.\n\n\n\n\n\n\nThe files we need for assembly are the \nmutant_R1.fastq\n and \nmutant_R2.fastq\n.\n\n\n\n\n\n\n(We don\nt need the reference genome sequences for this tutorial).\n\n\n\n\n\n\nThe reads are paired-end.\n\n\n\n\n\n\nEach read is 150 bases long. \n\n\n\n\n\n\nThe number of bases sequenced is equivalent to 19x the genome sequence of the wildtype strain. (Read coverage 19x - rather low!).\n\n\n\n\n\n\n\n\n\n\n\nClick on the View Data button (the \n) next to one of the FASTQ sequence files.\n\n\n\n\n\n\n\nAssemble reads with Spades\n\n\n\n\nWe will perform a \nde novo\n assembly of the mutant FASTQ reads into long contiguous sequences (in FASTA format.)\n\n\n\n\n\n\n\n\n\nGo to \nTools \n NGS Analysis \n NGS: Assembly \n spades\n\n\n\n\nSet the following parameters (leave other settings as they are):\n\n\n\n\nRun only Assembly\n: \nYes\n [the \nYes\n button should be darker grey]\n\n\nKmers to use separated by commas:\n \n33,55,91\n  [note: no spaces]  \n\n\nCoverage cutoff:\n \nauto\n  \n\n\nFiles \n Forward reads:\n \nmutant_R1.fastq\n  \n\n\nFiles \n Reverse reads:\n \nmutant_R2.fastq\n  \n\n\n\n\n\n\n\n\nYour tool interface should look like this:\n\n\n\n\n\n\n\n\n\n\nClick \nExecute\n\n\n\n\nExamine the output\n\n\n\n\nGalaxy is now running Spades on the reads for you.\n\n\n\n\nWhen it is finished, you will have five new files in your history.  \n\n\n\n\ntwo FASTA files of the resulting contigs and scaffolds\n\n\ntwo files for statistics about these\n\n\nthe Spades logfile\n\n\n\n\n\n\n\n\n\n\n\n\nClick on the View Data button \n on each of the files.\n\n\nNote that the short reads have been assembled into much longer contigs.\n\n\n(However, in this case, the contigs have not been assembled into larger scaffolds.)\n\n\nThe stats files will give you the length of each of the contigs.", 
            "title": "Genome assembly with Spades"
        }, 
        {
            "location": "/modules/spades/#assembly-using-spades", 
            "text": "Keywords: de novo assembly, Spades, Galaxy, Microbial Genomics Virtual Lab", 
            "title": "Assembly using Spades"
        }, 
        {
            "location": "/modules/spades/#background", 
            "text": "Spades is one of a number of  de novo  assemblers that use short read sets as input (e.g. Illumina Reads), and the assembly method is based on de Bruijn graphs. For information about Spades see this  link .   In this activity, we will perform a  de novo  assembly of a short read set using the Spades assembler. The output from Spades that we are interested in is a multiFASTA file that contains the draft genome sequence.", 
            "title": "Background"
        }, 
        {
            "location": "/modules/spades/#learning-objectives", 
            "text": "At the end of this tutorial you should be able to:    assemble the reads using Spades, and  examine the output assembly.", 
            "title": "Learning objectives"
        }, 
        {
            "location": "/modules/spades/#import-and-view-data", 
            "text": "See  here  for information about how to start with Galaxy, and  here  for the link to import the  Galaxy history of input files  for this tutorial, if you don t already have them in your history.   The read set for today is from an imaginary  Staphylococcus aureus  bacterium with a miniature genome.   The whole genome shotgun method used to sequence our mutant strain read set was produced on an Illumina DNA sequencing instrument.    The files we need for assembly are the  mutant_R1.fastq  and  mutant_R2.fastq .    (We don t need the reference genome sequences for this tutorial).    The reads are paired-end.    Each read is 150 bases long.     The number of bases sequenced is equivalent to 19x the genome sequence of the wildtype strain. (Read coverage 19x - rather low!).      Click on the View Data button (the  ) next to one of the FASTQ sequence files.", 
            "title": "Import and view data"
        }, 
        {
            "location": "/modules/spades/#assemble-reads-with-spades", 
            "text": "We will perform a  de novo  assembly of the mutant FASTQ reads into long contiguous sequences (in FASTA format.)     Go to  Tools   NGS Analysis   NGS: Assembly   spades   Set the following parameters (leave other settings as they are):   Run only Assembly :  Yes  [the  Yes  button should be darker grey]  Kmers to use separated by commas:   33,55,91   [note: no spaces]    Coverage cutoff:   auto     Files   Forward reads:   mutant_R1.fastq     Files   Reverse reads:   mutant_R2.fastq        Your tool interface should look like this:      Click  Execute", 
            "title": "Assemble reads with Spades"
        }, 
        {
            "location": "/modules/spades/#examine-the-output", 
            "text": "Galaxy is now running Spades on the reads for you.   When it is finished, you will have five new files in your history.     two FASTA files of the resulting contigs and scaffolds  two files for statistics about these  the Spades logfile       Click on the View Data button   on each of the files.  Note that the short reads have been assembled into much longer contigs.  (However, in this case, the contigs have not been assembled into larger scaffolds.)  The stats files will give you the length of each of the contigs.", 
            "title": "Examine the output"
        }, 
        {
            "location": "/modules/prokka/", 
            "text": "Genome annotation using Prokka\n\n\nKeywords: annotation, Prokka, JBrowse, Galaxy, Microbial Genomics Virtual Lab\n\n\nBackground\n\n\nIn this section we will use a software tool called Prokka to annotate the draft genome sequence produced in the previous \ntutorial\n. Prokka is a \u201cwrapper\u201d; it collects together several pieces of software (from various authors), and so avoids \u201cre-inventing the wheel\u201d.\n\n\nProkka finds and annotates features (both protein coding regions and RNA genes, i.e. tRNA, rRNA) present on on a sequence. Note, Prokka uses a two-step process for the annotation of protein coding regions: first, protein coding regions on the genome are identified using \nProdigal\n; second, the \nfunction\n of the encoded protein is predicted by similarity to proteins in one of many protein or protein domain databases. Prokka is a software tool that can be used to annotate bacterial, archaeal and viral genomes quickly, generating standard output files in GenBank, EMBL and gff formats. More information about Prokka can be found \nhere\n.\n\n\nLearning objectives\n\n\nAt the end of this tutorial you should be able to:\n\n\n\n\nload a genome assembly into Prokka\n\n\nannotate the assembly using Prokka\n\n\nexamine the annotated genome using JBrowse\n\n\n\n\nInput data\n\n\nProkka requires assembled contigs.\n\n\n\n\n\n\nIf you are continuing on from the previous workshop (\nAssembly with Spades\n), this file will be in your current history: \nSPAdes_contigs.fasta\n.\n\n\n\n\n\n\nAlternatively, get the file called \nassembled contigs\n from the \nTraining dataset page.\n\n\n\n\n\n\n\n\n\nRun Prokka\n\n\n\n\nIn Galaxy, go to \nTools \n NGS Analysis \n NGS: Annotation \n Prokka\n  \n\n\nSet the following parameters (leave everything else unchanged):\n\n\nContigs to annotate\n: \nSPAdes contigs (fasta)\n  \n\n\nLocus tag prefix (\nlocustag)\n: P\n\n\nForce GenBank/ENA/DDJB compliance (\ncompliant)\n: \nNo\n\n\nSequencing Centre ID (\ncentre)\n: V\n\n\nGenus Name\n: \nStaphylococcus\n  \n\n\nSpecies Name\n: \naureus\n  \n\n\nUse genus-specific BLAST database\n \nNo\n  \n\n\n\n\n\n\n\n\nYour tool interface should look like this:\n\n\n\n\n\n\nClick \nExecute\n  \n\n\n\n\nExamine the output\n\n\nFirst, enable \nScratchbook\n in Galaxy - this allows you to view several windows simultaneously. Click on the 3\n3 squares icon on the menu bar:\n\n\n\n\nOnce Prokka has finished, examine each of its output files.\n\n\n\n\nThe GFF and GBK files contain all of the information about the features annotated (in different formats.)\n\n\nThe \n.txt\n file contains a summary of the number of features annotated.\n\n\nThe \n.faa\n file contains the protein sequences of the genes annotated.\n\n\nThe \n.ffn\n file contains the nucleotide sequences of the genes annotated.\n\n\n\n\nView annotated features in JBrowse\n\n\nNow that we have annotated the draft genome sequence, we would like to view the sequence in the JBrowse genome viewer.\n\n\n\n\n\n\nGo to \nStatistics and Visualisation \n Graph/Display Data \n JBrowse\n (choose the top listed one; version 0.5.2).\n\n\n\n\n\n\nUnder \nJBrowse-in-Galaxy Action\n choose \nNew JBrowse Instance\n.\n\n\n\n\n\n\nUnder \nReference genome to display\n choose \nUse a genome from history\n.\n\n\n\n\n\n\nUnder \nFasta sequences\n choose \nProkka on data XX:fna\n. This sequence will be the reference against which annotations are displayed.\n\n\n\n\n\n\nFor \nProduce a Standalone Instance\n select \nYes\n.\n\n\n\n\n\n\nFor \nGenetic Code\n choose \n11: The Bacterial, Archaeal and Plant Plastid Code\n.\n\n\n\n\n\n\nClick \nInsert Track Group\n\n\n\n\n\n\nUnder \nTrack Category\n type in \ngene annotations\n.\n\n\n\n\n\n\nClick \nInsert Annotation Track\n\n\n\n\n\n\nFor \nTrack Type\n choose \nGFF/GFF3/BED/GBK Features\n\n\n\n\n\n\nFor \nGFF/GFF3/BED Track Data\n select \nProkka on data XX:gff\n  [Note: not wildtype.gff]\n\n\n\n\n\n\nUnder \nJBrowse Track Type[Advanced]\n select \nCanvas Features\n.\n\n\n\n\n\n\nClick on \nJBrowse Styling Options \n\n\n\n\n\n\nUnder \nJBrowse style.label\n correct the word \nprodcut\n to \nproduct\n.\n\n\n\n\n\n\nUnder \nTrack Visibility\n choose \nOn for new users\n.\n\n\n\n\n\n\nYour tool interface should look like this:\n\n\n\n\n\n\n\n\nClick \nExecute\n\n\n\n\n\n\nA new file will be created, called \nJBrowse on data XX and data XX - Complete\n. Click on the eye icon next to the file name. The JBrowse window will appear in the centre Galaxy panel.\n\n\n\n\n\n\nUnder \nAvailable Tracks\n on the left, tick the box for \nProkka on data XX:gff\n.\n\n\n\n\n\n\nSelect contig 6 in the drop down box. You can only see one contig displayed at a time.\n\n\n\n\n\n\n\n\n\n\n\n\nUse the plus and minus buttons to zoom in and out, and the arrows to move left or right (or click and drag within the window to move left or right).\n\n\n\n\n\n\nZoom in to see the reference sequence at the top. JBrowse displays the sequence and a 6-frame amino acid translation.\n\n\n\n\n\n\nZoomed in view:\n\n\n\n\n\n\nClick on a gene/feature annotation (the bars on the annotation track) to see more information.\n\n\ngene name\n\n\nproduct name\n\n\nyou can download the FASTA sequence by clicking on the disk icon.", 
            "title": "Genome annotation"
        }, 
        {
            "location": "/modules/prokka/#genome-annotation-using-prokka", 
            "text": "Keywords: annotation, Prokka, JBrowse, Galaxy, Microbial Genomics Virtual Lab", 
            "title": "Genome annotation using Prokka"
        }, 
        {
            "location": "/modules/prokka/#background", 
            "text": "In this section we will use a software tool called Prokka to annotate the draft genome sequence produced in the previous  tutorial . Prokka is a \u201cwrapper\u201d; it collects together several pieces of software (from various authors), and so avoids \u201cre-inventing the wheel\u201d.  Prokka finds and annotates features (both protein coding regions and RNA genes, i.e. tRNA, rRNA) present on on a sequence. Note, Prokka uses a two-step process for the annotation of protein coding regions: first, protein coding regions on the genome are identified using  Prodigal ; second, the  function  of the encoded protein is predicted by similarity to proteins in one of many protein or protein domain databases. Prokka is a software tool that can be used to annotate bacterial, archaeal and viral genomes quickly, generating standard output files in GenBank, EMBL and gff formats. More information about Prokka can be found  here .", 
            "title": "Background"
        }, 
        {
            "location": "/modules/prokka/#learning-objectives", 
            "text": "At the end of this tutorial you should be able to:   load a genome assembly into Prokka  annotate the assembly using Prokka  examine the annotated genome using JBrowse", 
            "title": "Learning objectives"
        }, 
        {
            "location": "/modules/prokka/#input-data", 
            "text": "Prokka requires assembled contigs.    If you are continuing on from the previous workshop ( Assembly with Spades ), this file will be in your current history:  SPAdes_contigs.fasta .    Alternatively, get the file called  assembled contigs  from the  Training dataset page.", 
            "title": "Input data"
        }, 
        {
            "location": "/modules/prokka/#run-prokka", 
            "text": "In Galaxy, go to  Tools   NGS Analysis   NGS: Annotation   Prokka     Set the following parameters (leave everything else unchanged):  Contigs to annotate :  SPAdes contigs (fasta)     Locus tag prefix ( locustag) : P  Force GenBank/ENA/DDJB compliance ( compliant) :  No  Sequencing Centre ID ( centre) : V  Genus Name :  Staphylococcus     Species Name :  aureus     Use genus-specific BLAST database   No        Your tool interface should look like this:    Click  Execute", 
            "title": "Run Prokka"
        }, 
        {
            "location": "/modules/prokka/#examine-the-output", 
            "text": "First, enable  Scratchbook  in Galaxy - this allows you to view several windows simultaneously. Click on the 3 3 squares icon on the menu bar:   Once Prokka has finished, examine each of its output files.   The GFF and GBK files contain all of the information about the features annotated (in different formats.)  The  .txt  file contains a summary of the number of features annotated.  The  .faa  file contains the protein sequences of the genes annotated.  The  .ffn  file contains the nucleotide sequences of the genes annotated.", 
            "title": "Examine the output"
        }, 
        {
            "location": "/modules/prokka/#view-annotated-features-in-jbrowse", 
            "text": "Now that we have annotated the draft genome sequence, we would like to view the sequence in the JBrowse genome viewer.    Go to  Statistics and Visualisation   Graph/Display Data   JBrowse  (choose the top listed one; version 0.5.2).    Under  JBrowse-in-Galaxy Action  choose  New JBrowse Instance .    Under  Reference genome to display  choose  Use a genome from history .    Under  Fasta sequences  choose  Prokka on data XX:fna . This sequence will be the reference against which annotations are displayed.    For  Produce a Standalone Instance  select  Yes .    For  Genetic Code  choose  11: The Bacterial, Archaeal and Plant Plastid Code .    Click  Insert Track Group    Under  Track Category  type in  gene annotations .    Click  Insert Annotation Track    For  Track Type  choose  GFF/GFF3/BED/GBK Features    For  GFF/GFF3/BED Track Data  select  Prokka on data XX:gff   [Note: not wildtype.gff]    Under  JBrowse Track Type[Advanced]  select  Canvas Features .    Click on  JBrowse Styling Options     Under  JBrowse style.label  correct the word  prodcut  to  product .    Under  Track Visibility  choose  On for new users .    Your tool interface should look like this:     Click  Execute    A new file will be created, called  JBrowse on data XX and data XX - Complete . Click on the eye icon next to the file name. The JBrowse window will appear in the centre Galaxy panel.    Under  Available Tracks  on the left, tick the box for  Prokka on data XX:gff .    Select contig 6 in the drop down box. You can only see one contig displayed at a time.       Use the plus and minus buttons to zoom in and out, and the arrows to move left or right (or click and drag within the window to move left or right).    Zoom in to see the reference sequence at the top. JBrowse displays the sequence and a 6-frame amino acid translation.    Zoomed in view:    Click on a gene/feature annotation (the bars on the annotation track) to see more information.  gene name  product name  you can download the FASTA sequence by clicking on the disk icon.", 
            "title": "View annotated features in JBrowse"
        }, 
        {
            "location": "/modules/snippy/", 
            "text": "Variant calling with Snippy\n\n\nKeywords: variant calling, SNP, Snippy, JBrowse, Galaxy, Microbial Genomics Virtual Lab\n\n\nBackground\n\n\nVariant calling is the process of identifying differences between two genome samples.\nUsually differences are limited to single nucleotide polymorphisms (SNPs) and small insertions and deletions (indels). Larger structural variation such as inversions, duplications and large deletions are not typically covered by \nvariant calling\n.\n\n\nLearning Objectives\n\n\n\n\nFind variants between a reference genome and a set of reads\n\n\nVisualise the SNP in context of the reads aligned to the genome\n\n\nDetermine the effect of those variants on genomic features\n\n\nUnderstand if the SNP is potentially affecting the phenotype\n\n\n\n\nPrepare reference\n\n\n\n\n\n\n\n\nFor variant calling, we need a reference genome that is of the same strain as the input sequence reads.\n\n\nFor this tutorial, our reference is the \nwildtype.gbk\n file and our reads are \nmutant_R1.fastq\n and \nmutant_R2.fastq\n.\n\n\nIf these files are not presently in your Galaxy history, import them from the \nTraining dataset page.\n\n\nCall variants with Snippy\n\n\n\n\nGo to \nTools \n NGS Analysis \n NGS: Variant Analysis \n snippy\n\n\nFor \nReference type\n select \nGenbank\n.\n\n\nThen for \nReference Genbank\n choose the \nwildtype.gbk\n file.\n\n\nFor \nSingle or Paired-end reads\n choose \nPaired\n.\n\n\nThen choose the first set of reads, \nmutant_R1.fastq\n and second set of reads, \nmutant_R2.fastq\n.\n\n\nFor \nCleanup the non-snp output files\n select \nNo\n.\n\n\n\n\nYour tool interface should look like this:\n\n\n\n\n\n\nClick \nExecute\n.\n\n\n\n\nExamine Snippy output\n\n\nFirst, enable \nScratchbook\n in Galaxy - this allows you to view several windows simultaneously. Click on the squares:\n\n\n\n\nFrom Snippy, there are 10 output files in various formats.\n\n\n\n\nGo to the file called \nsnippy on data XX, data XX and data XX table\n and click on the eye icon.\n\n\nWe can see a list of variants. Look in column 3 to see which types the variants are, such as a SNP or a deletion.\n\n\nLook at the third variant called. This is a T\nA mutation, causing a stop codon. Look at column 14: the product of this gene is a methicillin resistance protein. Methicillin is an antibiotic. What might be the result of such a mutation? \n\n\n\n\nView Snippy output in JBrowse\n\n\n\n\n\n\nGo to \nStatistics and Visualisation \n Graph/Display Data \n JBrowse\n (choose the top listed one; version 0.5.2).\n\n\n\n\n\n\nUnder \nJBrowse-in-Galaxy Action\n choose \nNew JBrowse Instance\n.\n\n\n\n\n\n\nUnder \nReference genome to display\n choose \nUse a genome from history\n.\n\n\n\n\n\n\nUnder \nFasta Sequence(s)\n choose \nwildtype.fna\n. This sequence will be the reference against which annotations are displayed.\n\n\n\n\n\n\nFor \nProduce a Standalone Instance\n select \nYes\n.\n\n\n\n\n\n\nFor \nGenetic Code\n choose \n11: The Bacterial, Archaeal and Plant Plastid Code\n.\n\n\n\n\n\n\nWe will now set up three different tracks - these are datasets displayed underneath the reference sequence (which is displayed as nucleotides in FASTA format). We will choose to display the sequence reads (the .bam file), the variants found by snippy (the .gff file) and the annotated reference genome (the wildtype.gff)\n\n\n\n\n\n\nTrack 1 - sequence reads\n\n\n\n\nClick \nInsert Track Group\n\n\nFor \nTrack Cateogry\n name it \nsequence reads\n\n\nClick \nInsert Annotation Track\n\n\nFor \nTrack Type\n choose \nBAM Pileups\n\n\nFor \nBAM Track Data\n select \nthe snippy bam file\n\n\nFor \nAutogenerate SNP Track\n select \nYes\n\n\nUnder \nTrack Visibility\n choose \nOn for new users\n.\n\n\n\n\nTrack 2 - variants\n\n\n\n\nClick \nInsert Track Group\n again\n\n\nFor \nTrack Category\n name it \nvariants\n\n\nClick \nInsert Annotation Track\n\n\nFor \nTrack Type\n choose \nGFF/GFF3/BED/GBK Features\n\n\nFor \nSNP Track Data\n select \nthe snippy snps gff file\n\n\nUnder \nTrack Visibility\n choose \nOn for new users\n.\n\n\n\n\nTrack 3 - annotated reference\n\n\n\n\nClick \nInsert Track Group\n again\n\n\nFor \n Track Category\n name it \nannotated reference\n\n\nClick \nInsert Annotation Track\n\n\nFor \nTrack Type\n choose \nGFF/GFF3/BED/GBK Features\n\n\nFor \nSNP Track Data\n select \nwildtype.gff\n\n\nUnder \nJBrowse Track Type[Advanced]\n select \nCanvas Features\n.\n\n\nClick on \nJBrowse Styling Options \n\n\nUnder \nJBrowse style.label\n correct the word \nprodcut\n to \nproduct\n.\n\n\nUnder \nJBrowse style.description\n type in \nproduct,note,description\n\n\n\n\nUnder \nTrack Visibility\n choose \nOn for new users\n.\n\n\n\n\n\n\nClick \nExecute\n\n\n\n\n\n\nA new file will be created, called \nJBrowse on data XX and data XX - Complete\n. Click on the eye icon next to the file name. The JBrowse window will appear in the centre Galaxy panel.\n\n\n\n\n\n\nOn the left, tick boxes display the tracks\n\n\n\n\n\n\nUse the minus button to zoom out to see:\n\n\n\n\nsequence reads and their coverage (the grey graph)\n\n\n\n\n\n\n\n\nUse the plus button to zoom in to see:\n\n\n\n\nprobable real variants (a whole column of snps)\n\n\nprobable errors (single one here and there)\n\n\n\n\n\n\n\n\n\n\n\n\nIn the coordinates box, type in \n47299\n and then \nGo\n to see the position of the SNP discussed above.\n\n\nthe correct codon at this position is TGT, coding for the amino acid Cysteine, in the middle row of the amino acid translations.\n\n\nthe mutation of T \n A turns this triplet into TGA, a stop codon.", 
            "title": "Variant finding"
        }, 
        {
            "location": "/modules/snippy/#variant-calling-with-snippy", 
            "text": "Keywords: variant calling, SNP, Snippy, JBrowse, Galaxy, Microbial Genomics Virtual Lab", 
            "title": "Variant calling with Snippy"
        }, 
        {
            "location": "/modules/snippy/#background", 
            "text": "Variant calling is the process of identifying differences between two genome samples.\nUsually differences are limited to single nucleotide polymorphisms (SNPs) and small insertions and deletions (indels). Larger structural variation such as inversions, duplications and large deletions are not typically covered by  variant calling .", 
            "title": "Background"
        }, 
        {
            "location": "/modules/snippy/#learning-objectives", 
            "text": "Find variants between a reference genome and a set of reads  Visualise the SNP in context of the reads aligned to the genome  Determine the effect of those variants on genomic features  Understand if the SNP is potentially affecting the phenotype", 
            "title": "Learning Objectives"
        }, 
        {
            "location": "/modules/snippy/#prepare-reference", 
            "text": "For variant calling, we need a reference genome that is of the same strain as the input sequence reads.  For this tutorial, our reference is the  wildtype.gbk  file and our reads are  mutant_R1.fastq  and  mutant_R2.fastq .  If these files are not presently in your Galaxy history, import them from the  Training dataset page.", 
            "title": "Prepare reference"
        }, 
        {
            "location": "/modules/snippy/#call-variants-with-snippy", 
            "text": "Go to  Tools   NGS Analysis   NGS: Variant Analysis   snippy  For  Reference type  select  Genbank .  Then for  Reference Genbank  choose the  wildtype.gbk  file.  For  Single or Paired-end reads  choose  Paired .  Then choose the first set of reads,  mutant_R1.fastq  and second set of reads,  mutant_R2.fastq .  For  Cleanup the non-snp output files  select  No .   Your tool interface should look like this:    Click  Execute .", 
            "title": "Call variants with Snippy"
        }, 
        {
            "location": "/modules/snippy/#examine-snippy-output", 
            "text": "First, enable  Scratchbook  in Galaxy - this allows you to view several windows simultaneously. Click on the squares:   From Snippy, there are 10 output files in various formats.   Go to the file called  snippy on data XX, data XX and data XX table  and click on the eye icon.  We can see a list of variants. Look in column 3 to see which types the variants are, such as a SNP or a deletion.  Look at the third variant called. This is a T A mutation, causing a stop codon. Look at column 14: the product of this gene is a methicillin resistance protein. Methicillin is an antibiotic. What might be the result of such a mutation?", 
            "title": "Examine Snippy output"
        }, 
        {
            "location": "/modules/snippy/#view-snippy-output-in-jbrowse", 
            "text": "Go to  Statistics and Visualisation   Graph/Display Data   JBrowse  (choose the top listed one; version 0.5.2).    Under  JBrowse-in-Galaxy Action  choose  New JBrowse Instance .    Under  Reference genome to display  choose  Use a genome from history .    Under  Fasta Sequence(s)  choose  wildtype.fna . This sequence will be the reference against which annotations are displayed.    For  Produce a Standalone Instance  select  Yes .    For  Genetic Code  choose  11: The Bacterial, Archaeal and Plant Plastid Code .    We will now set up three different tracks - these are datasets displayed underneath the reference sequence (which is displayed as nucleotides in FASTA format). We will choose to display the sequence reads (the .bam file), the variants found by snippy (the .gff file) and the annotated reference genome (the wildtype.gff)    Track 1 - sequence reads   Click  Insert Track Group  For  Track Cateogry  name it  sequence reads  Click  Insert Annotation Track  For  Track Type  choose  BAM Pileups  For  BAM Track Data  select  the snippy bam file  For  Autogenerate SNP Track  select  Yes  Under  Track Visibility  choose  On for new users .   Track 2 - variants   Click  Insert Track Group  again  For  Track Category  name it  variants  Click  Insert Annotation Track  For  Track Type  choose  GFF/GFF3/BED/GBK Features  For  SNP Track Data  select  the snippy snps gff file  Under  Track Visibility  choose  On for new users .   Track 3 - annotated reference   Click  Insert Track Group  again  For   Track Category  name it  annotated reference  Click  Insert Annotation Track  For  Track Type  choose  GFF/GFF3/BED/GBK Features  For  SNP Track Data  select  wildtype.gff  Under  JBrowse Track Type[Advanced]  select  Canvas Features .  Click on  JBrowse Styling Options   Under  JBrowse style.label  correct the word  prodcut  to  product .  Under  JBrowse style.description  type in  product,note,description   Under  Track Visibility  choose  On for new users .    Click  Execute    A new file will be created, called  JBrowse on data XX and data XX - Complete . Click on the eye icon next to the file name. The JBrowse window will appear in the centre Galaxy panel.    On the left, tick boxes display the tracks    Use the minus button to zoom out to see:   sequence reads and their coverage (the grey graph)     Use the plus button to zoom in to see:   probable real variants (a whole column of snps)  probable errors (single one here and there)       In the coordinates box, type in  47299  and then  Go  to see the position of the SNP discussed above.  the correct codon at this position is TGT, coding for the amino acid Cysteine, in the middle row of the amino acid translations.  the mutation of T   A turns this triplet into TGA, a stop codon.", 
            "title": "View Snippy output in JBrowse"
        }, 
        {
            "location": "/modules/pacbio/", 
            "text": "Assembly with PacBio data and SMRT Portal\n\n\nKeywords: de novo assembly, PacBio, PacificBiosciences, HGAP, SMRT Portal, Microbial Genomics Virtual Laboratory\n\n\nThis tutorial will show you how to assemble a bacterial genome \nde novo\n, using the PacBio SMRT Portal on the mGVL. We will use an analysis pipeline called HGAP, the Hierarchical Genome Assembly Process.\n\n\n\n\n\nStart\n\n\n\n\nOpen your mGVL dashboard.\n\n\nYou should see SMRT Portal as one of the instance services on your GVL dashboard.\n\n\nOpen up the SMRT portal web link (to the right) and register/log on.\n\n\n\n\nInput\n\n\n\n\n\nWe will use a dataset from a \nStreptococcus pyogenes\n bacteria.\n\n\nIf this has already been loaded onto SMRT portal (e.g. for use during a workshop), proceed to the next step (\nAssembly\n).\n\n\nOtherwise:\n\n\n\n\nLoad the PacBio data (your own, or the training dataset) onto your GVL.\n\n\nIn the SMRT Portal, go to \nDesign Job\n, the top left tab.\n\n\nGo to \nImport and Manage\n.\n\n\n\nClick \nImport SMRT cells\n.\n\n\n\nWork out where you put the data on your GVL, and make sure the file path is showing.\n\n\nIf not, click \nAdd\n and enter the file path to the data.\n\n\n\n\n\n\nClick on the file path and then \nScan\n to check for new data.\n\n\n\n\nAssembly\n\n\nHGAP process overview\n\n\nWe will use the Hierarchical Genome Assembly Process (HGAP). This flowchart shows the steps in the process:\n\n\n\n\nSet up job\n\n\n\n\nIn the SMRT Portal, go to the top left tab, \nDesign Job\n.\n\n\nGo to \nCreate New\n.\n\n\nAn \nAnalysis\n window should appear. Check the box next to \nDe novo assembly\n, then \nNext\n.\n\n\nUnder \nJob Name\n enter a name.\n\n\nUnder \nProtocols\n choose \nRS_HGAP_Assembly.3\n.\n\n\nThere is an ellipsis underneath \nProtocols\n - click on the ellipsis.\n\n\n\n\n\n\nThis brings up the settings. Click on \nAssembly\n.\n\n\n\n\nFor \nCompute Minimum Seed Read Length\n: ensure box is ticked\n\n\nFor \nNumber of Seed Read Chunks\n: enter \n12\n\n\nChange the \nGenome Size\n to an approximately correct size for the species. For \nS. pyogenes\n, enter 1800000.\n\n\nFor \nTarget Coverage\n: enter \n10\n\n\nFor \nOverlapper Error Rate\n: enter \n0.04\n\n\nLeave all other settings as they are.\n\n\nClick \nApply\n\n\n\n\nYour protocol window should look like this:\n\n\n\n\n\n\n\n\nClick \nOk\n.  \n\n\n\n\n\n\nIn the \nSMRT Cells Available\n window, select the file to be used. Click on the arrow to transfer these files to the SMRT Cells in Job window.\n\n\n\n\n\n\n\n\n\n\nClick \nSave\n (bottom right hand side).\n\n\nNext to \nSave\n, click \nStart\n.\n\n\nThe \nMonitor Jobs\n window should open.\n\n\nAs each step proceeds, new items will appear under the \nReports\n and \nData\n tabs on the left.\n\n\n\n\n\n\n\n\n\n\nInputs and Outputs\n\n\nThe connections between the names of assembly stages and outputs is not always clear. This flowchart shows how each stage of the HGAP process corresponds to protocol window names and outputs:\n\n\n\n\nResults\n\n\nIf the job is still running, click on the centre tab \nMonitor Jobs\n. Otherwise, click on the top right tab, \nView Data\n.\n\n\n\n\nDouble click on the job name to open its reports.\n\n\nClick on different \nReports\n in the left hand panel.\n\n\n\n\nThings to look at:\n\n\nGeneral: Filtering (polymerase reads)\n\n\n\n\nnumber of reads post-filter\n\n\nread length (=average)\n\n\n\n\nGeneral: Subread Filtering (subreads)\n\n\n\n\nnumber of reads post-filter\n\n\nread length (average)\n\n\n\n\nAssembly: Pre-Assembly (pre-assembled reads)\n\n\n\n\nlength cutoff (the computed minimum seed read length)\n\n\nread length (average)\n\n\n\n\nAssembly: Corrections\n\n\nConsensus calling results:\n\n\n\n\nConsensus concordance should be \n 99%.\n\n\n\n\nGraph: corrections across reference:\n\n\n\n\nWith the first run of polishing, we expect a lot of corrections but they should be randomly distributed.\n\n\n\n\nAssembly: Top Corrections\n\n\nThis is a list of all the corrections made.\n\n\n\n\nIf more than two corrections (with confidence \n 50), repeat polishing (see next section \nFurther polishing\n).\n\n\n\n\nResequencing: Coverage\n\n\nCoverage across reference:\n\n\n\n\ndiscard contigs \n20X coverage\n\n\nothers should have fairly consistent coverage.\n\n\nspikes could be collapsed repeats.\n\n\nvalleys could be mis-assembly - e.g. draft assembly was incorrect and so remapped reads didn\nt support this part of the assembly.\n\n\n\n\nGraph: Depth of Coverage:\n\n\nNumber\n of reference regions vs coverage. \n\n\nAssembly: Polished Assembly\n\n\n\n\nnumber of contigs\n\n\nmax contig length\n\n\ngraph: confidence vs depth. multi-copy plasmids may have higher coverage.\n\n\n\n\nFurther Polishing\n\n\nDuring polishing, raw reads are used to correct the assembly.\nDuring HGAP, the assembly was polished once but may need further corrections.\n\n\n\n\nFrom the previous step, Go to \nData \n Assembly \n Polished Assembly\n and download the FASTA file by clicking on it.\n\n\nUnzip the .gz file\n\n\n\n\n\n\nGo to \nDesign Job \n Import and Manage\n and click \nNew\n on the bottom right hand side. Then, select that FASTA assembly file to upload.\n\n\ncreates a new reference.\n\n\n\n\n\n\nGo to \nDesign Job \n Create New\n\n\nchoose reference-based\n\n\nSelect protocol: RS_Resequencing.1\n\n\nLeave all settings.\n\n\nSelect your reference from the drop down menu.\n\n\nClick \nSave\n and \nStart\n.\n\n\n\n\n\n\nExamine the output assembly and repeat if necessary (e.g. if \n 2 corrections with \n50 confidence).\n\n\n\n\nOutput\n\n\nThe polished assembly as a FASTA file.\n\n\n\n\ndownload to local computer; or\n\n\nopen file in (GVL) Galaxy; or\n\n\nopen file in GVL command line: and perform further analysis.\n\n\n\n\nNext\n\n\nFurther options:\n\n\n\n\ncorrect with Illumina reads\n\n\ncircularise\n\n\nannotate\n\n\n\n\nLinks to more information\n\n\nPacBio \nE. coli\n data set\n\n\nHGAP overview\n\n\nA full ist of reports and terminology\n\n\nVideo overview of HGAP on SMRT portal\n\n\nMore about the SMRT bell template", 
            "title": "Assembly with SMRT portal"
        }, 
        {
            "location": "/modules/pacbio/#assembly-with-pacbio-data-and-smrt-portal", 
            "text": "Keywords: de novo assembly, PacBio, PacificBiosciences, HGAP, SMRT Portal, Microbial Genomics Virtual Laboratory  This tutorial will show you how to assemble a bacterial genome  de novo , using the PacBio SMRT Portal on the mGVL. We will use an analysis pipeline called HGAP, the Hierarchical Genome Assembly Process.", 
            "title": "Assembly with PacBio data and SMRT Portal"
        }, 
        {
            "location": "/modules/pacbio/#start", 
            "text": "Open your mGVL dashboard.  You should see SMRT Portal as one of the instance services on your GVL dashboard.  Open up the SMRT portal web link (to the right) and register/log on.", 
            "title": "Start"
        }, 
        {
            "location": "/modules/pacbio/#input", 
            "text": "We will use a dataset from a  Streptococcus pyogenes  bacteria.  If this has already been loaded onto SMRT portal (e.g. for use during a workshop), proceed to the next step ( Assembly ).  Otherwise:   Load the PacBio data (your own, or the training dataset) onto your GVL.  In the SMRT Portal, go to  Design Job , the top left tab.  Go to  Import and Manage .  Click  Import SMRT cells .  Work out where you put the data on your GVL, and make sure the file path is showing.  If not, click  Add  and enter the file path to the data.    Click on the file path and then  Scan  to check for new data.", 
            "title": "Input"
        }, 
        {
            "location": "/modules/pacbio/#assembly", 
            "text": "", 
            "title": "Assembly"
        }, 
        {
            "location": "/modules/pacbio/#hgap-process-overview", 
            "text": "We will use the Hierarchical Genome Assembly Process (HGAP). This flowchart shows the steps in the process:", 
            "title": "HGAP process overview"
        }, 
        {
            "location": "/modules/pacbio/#set-up-job", 
            "text": "In the SMRT Portal, go to the top left tab,  Design Job .  Go to  Create New .  An  Analysis  window should appear. Check the box next to  De novo assembly , then  Next .  Under  Job Name  enter a name.  Under  Protocols  choose  RS_HGAP_Assembly.3 .  There is an ellipsis underneath  Protocols  - click on the ellipsis.    This brings up the settings. Click on  Assembly .   For  Compute Minimum Seed Read Length : ensure box is ticked  For  Number of Seed Read Chunks : enter  12  Change the  Genome Size  to an approximately correct size for the species. For  S. pyogenes , enter 1800000.  For  Target Coverage : enter  10  For  Overlapper Error Rate : enter  0.04  Leave all other settings as they are.  Click  Apply   Your protocol window should look like this:     Click  Ok .      In the  SMRT Cells Available  window, select the file to be used. Click on the arrow to transfer these files to the SMRT Cells in Job window.      Click  Save  (bottom right hand side).  Next to  Save , click  Start .  The  Monitor Jobs  window should open.  As each step proceeds, new items will appear under the  Reports  and  Data  tabs on the left.", 
            "title": "Set up job"
        }, 
        {
            "location": "/modules/pacbio/#inputs-and-outputs", 
            "text": "The connections between the names of assembly stages and outputs is not always clear. This flowchart shows how each stage of the HGAP process corresponds to protocol window names and outputs:", 
            "title": "Inputs and Outputs"
        }, 
        {
            "location": "/modules/pacbio/#results", 
            "text": "If the job is still running, click on the centre tab  Monitor Jobs . Otherwise, click on the top right tab,  View Data .   Double click on the job name to open its reports.  Click on different  Reports  in the left hand panel.   Things to look at:  General: Filtering (polymerase reads)   number of reads post-filter  read length (=average)   General: Subread Filtering (subreads)   number of reads post-filter  read length (average)   Assembly: Pre-Assembly (pre-assembled reads)   length cutoff (the computed minimum seed read length)  read length (average)   Assembly: Corrections  Consensus calling results:   Consensus concordance should be   99%.   Graph: corrections across reference:   With the first run of polishing, we expect a lot of corrections but they should be randomly distributed.   Assembly: Top Corrections  This is a list of all the corrections made.   If more than two corrections (with confidence   50), repeat polishing (see next section  Further polishing ).   Resequencing: Coverage  Coverage across reference:   discard contigs  20X coverage  others should have fairly consistent coverage.  spikes could be collapsed repeats.  valleys could be mis-assembly - e.g. draft assembly was incorrect and so remapped reads didn t support this part of the assembly.   Graph: Depth of Coverage:  Number  of reference regions vs coverage.   Assembly: Polished Assembly   number of contigs  max contig length  graph: confidence vs depth. multi-copy plasmids may have higher coverage.", 
            "title": "Results"
        }, 
        {
            "location": "/modules/pacbio/#further-polishing", 
            "text": "During polishing, raw reads are used to correct the assembly.\nDuring HGAP, the assembly was polished once but may need further corrections.   From the previous step, Go to  Data   Assembly   Polished Assembly  and download the FASTA file by clicking on it.  Unzip the .gz file    Go to  Design Job   Import and Manage  and click  New  on the bottom right hand side. Then, select that FASTA assembly file to upload.  creates a new reference.    Go to  Design Job   Create New  choose reference-based  Select protocol: RS_Resequencing.1  Leave all settings.  Select your reference from the drop down menu.  Click  Save  and  Start .    Examine the output assembly and repeat if necessary (e.g. if   2 corrections with  50 confidence).", 
            "title": "Further Polishing"
        }, 
        {
            "location": "/modules/pacbio/#output", 
            "text": "The polished assembly as a FASTA file.   download to local computer; or  open file in (GVL) Galaxy; or  open file in GVL command line: and perform further analysis.", 
            "title": "Output"
        }, 
        {
            "location": "/modules/pacbio/#next", 
            "text": "Further options:   correct with Illumina reads  circularise  annotate", 
            "title": "Next"
        }, 
        {
            "location": "/modules/pacbio/#links-to-more-information", 
            "text": "PacBio  E. coli  data set  HGAP overview  A full ist of reports and terminology  Video overview of HGAP on SMRT portal  More about the SMRT bell template", 
            "title": "Links to more information"
        }, 
        {
            "location": "/modules/cmdline_assembly/", 
            "text": "Pacbio reads: assembly with command line tools\n\n\nKeywords: de novo assembly, PacBio, PacificBiosciences, Illumina, command line, Canu, Circlator, BWA, Spades, Pilon, Microbial Genomics Virtual Laboratory\n\n\nThis tutorial demonstrates how to use long Pacbio sequence reads to assemble a bacterial genome and plasmids, including correcting the assembly with short Illumina reads.\n\n\nLearning objectives\n\n\nAt the end of this tutorial, be able to use command line tools to produce a bacterial genome assembly using the following workflow:\n\n\n\n\nget data\n\n\nassemble long (Pacbio) reads\n\n\ntrim overhangs and circularise\n\n\nsearch for smaller plasmids\n\n\ncorrect with short (Illumina) reads\n\n\n\n\nOverview\n\n\nSimplified version of workflow:\n\n\n\n\nGet data\n\n\n\n\nOpen the mGVL command line\n\n\nNavigate to or create the directory in which you want to work.\ne.g.\n\n\n\n\nmkdir staph\ncd staph\n\n\n\n\nIf you already have the files ready, skip forward to next section, \nAssemble\n.\n\n\nFind the Pacbio files for this sample\n\n\n\n\nIf the files are already on your server, you can symlink by using\n\n\n\n\nln -s real_file_path chosen_symlink_name\n\n\n\n\n\n\nAlternatively, obtain the input files from elsewhere, e.g. from the BPA portal. (You will need a password.)\n\n\nPacbio files are often stored in the format: Sample_name/Cell_name/Analysis_Results/long_file_name_1.fastq.gz\n\n\nWe will use the \nlongfilename.subreads.fastq.gz\n files.\n\n\nThe reads are usually split into three separate files because they are so large.\n\n\nRight click on the first \nsubreads.fastq.gz\n file and \ncopy link address\n.\n\n\nIn the command line, type:\n\n\n\n\nwget --user username --password password [paste link URL for file]\n\n\n\n\n\n\nRepeat for the other two \nsubreads.fastq.gz\n files.\n\n\n\n\nJoin Pacbio fastq files\n\n\n\n\nIf the files are gzipped, type:\n\n\n\n\ncat filepath/filep0.*.subreads.fastq.gz \n subreads.fastq.gz\n\n\n\n\n\n\nIf the files are not gzipped, type:\n\n\n\n\ncat filepath/filep0.*.subreads.fastq | gzip \n subreads.fastq.gz\n\n\n\n\n\n\nWe now have a file called \nsubreads.fastq.gz\n.\n\n\n\n\nFind the Illumina files for this sample\n\n\n\n\nWe will also use 2 x Illumina (Miseq) fastq.gz files.\n\n\nThese are the \nR1.fastq.gz\n and \nR2.fastq.gz\n files.\n\n\nRight click on the file name and \ncopy link address\n.\n\n\nIn the command line, type:\n\n\n\n\nwget --user username --password password [paste link URL for file]\n\n\n\n\n\n\nRepeat for the other read.fastq.gz file.\n\n\nShorten the name of each of these files:\n\n\n\n\nmv longfilename_R1.fastq.gz R1.fastq.gz\nmv longfilename_R2.fastq.gz R2.fastq.gz\n\n\n\n\nView files\n\n\n\n\nType \nls\n to display the folder contents.\n\n\n\n\nls\n\n\n\n\n\n\nThe 3 files we will use in this analysis are:\n\n\nsubreads.fastq.gz\n (the Pacbio reads)\n\n\nR1.fastq.gz\n and \nR2.fastq.gz\n (the Illumina reads)\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this tutorial we will use \nStaphylococcus aureus\n sample 25745.\n\n\n\n\nAssemble\n\n\n\n\nWe will use the assembly software called \nCanu\n.\n\n\nRun Canu with these commands:\n\n\n\n\ncanu -p staph -d output genomeSize=2.8m -pacbio-raw subreads.fastq\n\n\n\n\n\n\nstaph\n is the prefix given to output files\n\n\noutput\n is the name of the output directory\n\n\ngenomeSize\n only has to be approximate.\n\n\ne.g. \nStaphylococcus aureus\n, 2.8m\n\n\ne.g. \nStreptococcus pyogenes\n, 1.8m\n\n\n\n\n\n\n\n\nthe \nreads\n can be unzipped or .gz\n\n\n\n\n\n\nCanu will correct, trim and assemble the reads.\n\n\n\n\nThis will take ~ 30 minutes.\n\n\n\n\nCheck the output\n\n\ncd output\n\n\n\n\n\n\nThe \nstaph.contigs.fasta\n are the assembled sequences.\n\n\nThe \nstaph.unassembled.fasta\n are the reads that could not be assembled.\n\n\nThe \nstaph.correctedReads.fasta.gz\n are the corrected Pacbio reads that were used in the assembly.\n\n\nThe \nstaph.file.gfa\n is the graph of the assembly.\n\n\nDisplay summary information about the contigs:\n\n\n\n\ninfoseq staph.contigs.fasta\n\n\n\n\n\n\n\n\n(infoseq is a tool from \nEMBOSS\n)\n\n\n\n\n\n\nThis will show the number of contigs, e.g.\n\n\n\n\n\n\n    - tig00000000   dna 2746242\n    - tig00000001   dna 48500\n\n\n\n\nChange Canu parameters if required\n\n\n\n\nIf the assembly is poor with many contigs, re-run Canu with extra sensitivity parameters; e.g.\n\n\n\n\ncanu -p prefix -d outdir corMhapSensitivity=high corMinCoverage=0 genomeSize=2.8m -pacbio-raw subreads.fastq\n\n\n\n\nTrim and circularise\n\n\nRun Circlator\n\n\nCirclator identifies and trims overhangs (on chromosomes and plasmids) and orients the start position at an appropriate gene (e.g. dnaA). It takes in the assembled contigs from Canu, as well as the corrected reads prepared by Canu.\n\n\nTo run:\n\n\ncirclator all --threads 8 --verbose --b2r_length_cutoff 20000 ../staph.contigs.fasta ../staph.corrected.reads.fastq.gz circlator_all_output\n\n\n\n\n\n\nthreads\n is the number of cores\n\n\nverbose\n prints progress information to the screen\n\n\nb2r_length_cutoff\n using approximately 2X average read length (could be omitted at first; if all contigs don\nt circularise, include this option to see if any improvement)\n\n\n../staph.contigs.fasta\n is the file path to the input multi-fasta assembly\n\n\n../staph.corrected.reads.fastq.gz\n is the file path to the corrected Pacbio reads\n\n\ncirclator_all_output\n is the name of the output directory.\n\n\n\n\nCheck the output: were the contigs circularised:\n\n\nless 04.merge.circularise.log\n\n\n\n\nWhere were the contigs oriented (which gene):\n\n\nless 06.fixstart.log\n\n\n\n\nWhat are the trimmed contig sizes:\n\n\nfa -f 06.fixstart.fasta\n\n\n\n\nThe trimmed contigs are in the file called \n06.fixstart.fasta\n. Re-name it \ncontig_1_2.fa\n:\n\n\nmv 06.fixstart.fasta contig_1_2.fa\n\n\n\n\nFind smaller plasmids\n\n\nPacbio reads are long, and may have been longer than small plasmids. We will look for any small plasmids using the Illumina reads.\n\n\nThis section involves several steps:\n\n\n\n\nUse the multifasta canu-circlator output of trimmed assembly contigs.\n\n\nMap all the Illumina reads against these Pacbio assembled contigs.\n\n\nExtract any reads that \ndidn\nt\n map and assemble them together: this could be a plasmid, or part of a plasmid.\n\n\nLook for overhang: if found, trim. If not, continue:\n\n\nSearch Genbank for any matching proteins: a replication protein found.  \n\n\nAssemble all the Illumina reads and produce an assembly graph.\n\n\nSearch the graph for a match to the replication protein and its adjoining regions.\n\n\nExtract this longer sequence from the Illumina assembly: this is the small plasmid.\n\n\nCheck for overhang in this plasmid and trim.\n\n\n\n\nAlign Illumina with BWA\n\n\n\n\nAlign illumina reads to these contigs\n\n\nFirst, index the contigs file\n\n\n\n\nbwa index contig_1_2.fa\n\n\n\n\n\n\nthen, align using bwa mem\n\n\n\n\nbwa mem -t 8 contig_1_2.fa R1.fastq.gz R2.fastq.gz | samtools sort \n aln.bam\n\n\n\n\n\n\nbwa mem\n is the alignment tool\n\n\n-t 8\n is the number of cores\n\n\ncontig_1_2.fa\n is the input assembly file\n\n\nR1.fastq.gz R2.fastq.gz\n are the Illumina reads\n\n\n | samtools sort\n pipes the output to samtools to sort\n\n\n aln.bam\n sends the alignment to the file \naln.bam\n\n\n\n\nExtract unmapped Illumina reads\n\n\n\n\nIndex the alignment file\n\n\n\n\nsamtools index aln.bam\n\n\n\n\n\n\nextract the fastq files from the bam alignment - those reads that were unmapped to the Pacbio alignment - and save them in various \nunmapped\n files:\n\n\n\n\nsamtools fastq -f 4 -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq aln.bam\n\n\n\n\n\n\nfastq\n is a command that coverts a \n.bam\n file into fastq format\n\n\n-f 4\n : only output unmapped reads\n\n\n-1\n : put R1 reads into a file called \nunmapped.R1.fastq\n\n\n-2\n : put R2 reads into a file called \nunmapped.R2.fastq\n\n\n-s\n : put singleton reads into a file called \nunmapped.RS.fastq\n\n\naln.bam\n : input alignment file\n\n\n\n\nWe now have three files of the unampped reads:\n\n\n\n\n unmapped.R1.fastq\n\n\n unmapped.R2.fastq\n\n\n unmapped.RS.fastq\n\n\n\n\nAssemble the unmapped reads\n\n\n\n\nassemble with spades\n\n\n\n\nspades.py -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq --careful --cov-cutoff auto -o spades_assembly\n\n\n\n\n\n\n-1\n is input file forward\n\n\n-2\n is input file reverse\n\n\n-s\n is unpaired\n\n\n careful\n : minimizes mismatches and short indels\n\n\n cov-cutoff auto\n : computes the coverage threshold (rather than the default setting, \noff\n)\n\n\n-o\n is the output directory\n\n\n\n\ncd spades_assembly\ninfoseq contigs.fasta\n\n\n\n\n\n\nshows how many assembled:\n\n\ne.g. no=135\n\n\nmax = 2229\n\n\n\n\n\n\nsort fasta by size of seqs:\n\n\n\n\nsizeseq\ninput sequence set: contigs.fasta\nreturn longest sequence first [N]: Y\noutput sequence(s) [contigs.fasta]: sorted_contigs.fasta\n\n\n\n\nPrint the first row of each seq to see coverage:\n\n\ngrep cov sorted_contigs.fasta  \n\n\n\n\n\n\nresult: NODE_1_length_2229_cov_610.583\n\n\nlongest contig is 2229 and high coverage\n\n\n\n\n\n\nall the nodes are listed\n\n\nsee if any other nodes have high coverage\n\n\ne.g. NODE_135_length_78_cov_579\n\n\n\n\n\n\nlook at the sequence of this contig:\n\n\n\n\ntail sorted_contigs.fasta\n\n\n\n\n\n\nThis is a homopolymer, so disregard.\n\n\nWe will extract the first sequence (NODE_1):\n\n\n\n\nsamtools faidx sorted_contigs.fasta\nsamtools faidx sorted_contigs.fasta NODE_1_length_2229_cov_610.583 \n contig3.fa\n\n\n\n\n\n\nthis is now saved as \ncontig3.fa\n\n\nopen this file in nano, make the header \ncontig3\n, save.\n\n\n\n\nInvestigate the small plasmid (contig3)\n\n\n\n\nBlast the start of contig3 against itself\n\n\nTake the start of the contig:\n\n\n\n\nhead -n 10 contig3.fa \n contig3.fa.head\n\n\n\n\n\n\nWe want to see if it matches the end (overhang)\n\n\nFormat the assembly file for blast:\n\n\n\n\nmakeblastdb -in contig3.fa -dbtype nucl\n\n\n\n\n\n\nblast the start of the assembly (.head file) against all of the assembly:\n\n\n\n\nblastn -query contig3.fa.head -db contig3.fa -evalue 1e-3 -dust no -out contig3.bls\n\n\n\n\n\n\nlook at \ncontig3.bls\n to see hits:\n\n\n\n\nless contig3.bls\n\n\n\n\n\n\nthe first hit is against itself, as expected\n\n\nthere are no few further hits, so we assume there is no overhang that needs trimming.\n\n\nhowever, the sequence is likely then to be longer than this.\n\n\n\n\nless contig3.fa\n\n\n\n\n\n\nCopy the sequence\n\n\nGo to NCBI: \nhttps://blast.ncbi.nlm.nih.gov/Blast.cgi\n; choose blastx\n\n\nPaste the sequence from contig3.fa\n\n\nChoose genetic code = 11\n\n\nBlast\n\n\n\nThis hits a replication (plasmid) protein. Hypothesise that   this is a small plasmid; search for the entire sequence within the assembly of all the Illumina reads (next step).\n\n\n\n\nAssemble \nall\n the illumina reads\n\n\n\n\nAssemble all the Illumina reads with spades (not just those reads that did not map to the Pacbio assembly).\n\n\n\n\nspades.py -1 R1.fastq -2 R2.fastq --careful --cov-cutoff auto -o spades_assembly_all_illumina\n\n\n\n\n\n\n\nNavigate to the output:\n\n\ncd spades_assembly_all_illumina\n\n\n\n\n\n\nin here is the \nassembly_graph.fastg\n\n\nTransfer this file to your local computer (e.g. using the file transfer program \nCyberduck\n).\n\n\nExamine the assembly in the program \nBandage\n.\n\n\nFile: Load graph: \nassembly_graph.fastg\n\n\nIn the left hand panel, click \nDraw graph\n\n\nYour assembly graph may look like this:\n\n\n\n\n\n\n\n\n\nBlast the small plasmid sequence in this assembly\n\n\n\n\nIn the left hand panel: Blast: create/view BLAST search\n\n\nBuild blast database\n\n\nPaste in the sequence of contig3\n\n\nBlast\n\n\nThe main hit is around node 10.\n\n\n\n\n\n\n\n\nGo to the main Bandage window\n\n\n\n\nfind nodes\n in right hand panel - 10\n\n\nThis node is slightly longer: 2373: this could be the plasmid\n\n\nExtract this node in fasta format: In the top panel, go to Output: Save selected node sequences; save as \ncontig3b.fa\n\n\nOpen this file in nano and change the header to \ncontig3b\n, save.\n\n\n\n\n\n\n\n\nTrim small plasmid\n\n\n\n\nTake the start of the sequence and see if it matches the end:\n\n\n\n\nhead -n 10 contig3b.fa \n contig3b.fa.head\nmakeblastdb -in contig3b.fa -dbtype nucl\nblastn -query contig3b.fa.head -db contig3b.fa -evalue 1e-3 -dust no -out contig3b.bls\nless contig3b.bls\n\n\n\n\n\n\nThe first hit is against the start of the chromosome, as expected.\n\n\nThe last hit starts at position 2253; we will trim the plasmid to position 2252\n\n\nIndex the contig3b.fa file:\n\n\n\n\nsamtools faidx contig3b.fa\n\n\n\n\n\n\nTrim:\n\n\n\n\nsamtools faidx contig3b.fa contig3b:1-2252 \n contig3b.fa.trimmed\n\n\n\n\n\n\nOpen this file in nano and change the header to \ncontig3b\n, save.\n\n\nWe now have a trimmed contig3b.\n\n\n\n\nCollect all contigs in one file\n\n\ncat contig_1_2.fa contig3b.fa.trimmed \n all_contigs.fa\n\n\n\n\n\n\nSee the three contigs and sizes:\n\n\n\n\ninfoseq all_contigs.fa\n\n\n\n\nCorrect\n\n\nWe will correct the Pacbio assembly, first with Pacbio corrected reads (from Canu) and then with Illumina reads.\n\n\n\n\ninputs:\n\n\nDraft Pacbio assembly (overhang trimmed from each of the three replicons)\n\n\ncorrected (and trimmed) Pacbio reads from Canu.\n\n\nillumina reads (aligned to Pacbio assembly: in bam format)\n\n\n\n\n\n\noutput: corrected assembly\n\n\n\n\n\n\n\n1. Correct with Pacbio corrected/trimmed reads\n\n\nAlign reads to assembly:\n\n\nbwa index all_contigs.fa\nbwa mem -t 32 all_contigs.fa canu.trimmedReads.fasta.gz | samtools sort \n aln.bam\nsamtools index aln.bam\nsamtools faidx all_contigs.fa\n\n\n\n\n\n\n-t\n is the number of cores (e.g. 8)\n\n\nto find out how many you have, grep -c processor /proc/cpuinfo\n\n\n\n\n\n\nnow we have an alignment file to use in pilon: \naln.bam\n\n\n\n\nRun pilon:\n\n\npilon --genome all_contigs.fa --unpaired aln.bam --output pilon1 --fix all --mindepth 0.5 --changes --verbose\n\n\n\n\n\n\ngenome\n is the name of the input assembly to be corrected\n\n\nunpaired\n is the alignment of the reads against the assembly (use \nunpaired\n instead of \nfrags\n where alignment is from pacbio reads)\n\n\noutput\n is the name of the output prefix\n\n\nfix\n is an option for types of corrections\n\n\nmindepth\n gives a minimum read depth to use\n\n\nchanges\n produces an output file of the changes made\n\n\nverbose\n prints information to the screen during the run\n\n\nif you are using pilon on a different machine and you want to specify the number of CPUs, type in \nthreads\n number (e.g. 32).\n\n\n\n\nLook at the .changes file:\n\n\nless pilon1.changes | column -t\n\n\n\n\n\n\nThis shows the corrections made by Pilon:\n\n\n\n\n\n\n\n\nLook at the details of the .fasta file:\n\n\n\n\ninfoseq pilon1.fasta\n\n\n\n\n2. Correct with Illumina reads\n\n\nInput: the corrected assembly from the previous step: \npilon1.fasta\n\n\nAlign the Illumina reads to this assembly:\n\n\nbwa index pilon1.fasta\nbwa mem -t 32 pilon1.fasta R1.fastq.gz R2.fastq.gz | samtools sort \n aln.bam\nsamtools index aln.bam\nsamtools faidx pilon1.fasta\n\n\n\n\nLook at how the illumina reads are aligned:\n\n\nsamtools tview -p contig1 aln.bam pilon1.fasta\n\n\n\n\nnote: \ncontig1\n is the name of the contig to view; e.g. tig00000000.\n\n\nRun pilon (note: use \nfrags\n this time instead of \nunpaired\n):\n\n\npilon --genome pilon1.fa --frags aln.bam --output pilon2 --fix all --mindepth 0.5 --changes --verbose\n\n\n\n\nLook at the changes file:\n\n\nless pilon2.changes\n\n\n\n\nLook at the .fasta file:\n\n\nless pilon2.fasta\n\n\n\n\nIf there are more than 2 changes, run Pilon again, using the pilon2.fasta file as the input assembly, and the Illumina reads to correct.\n\n\nFinal output:\n\n\n\n\nthe corrected genome assembly of \nStaphylococcus aureus\n in .fasta format, containing three contigs: chromosome, large plasmid and small plasmid.  \n\n\n\n\nNext\n\n\nFurther analyses:\n\n\n\n\nAnnotate with Prokka.\n\n\nComparative genomics, e.g. with Roary.\n\n\n\n\nLinks:\n\n\n\n\nDetails of bas.h5 files\n\n\nCanu \nmanual\n and \ngitub repository\n\n\nCirclator \narticle\n and \ngithub repository\n\n\nPilon \narticle\n and \ngithub repository\n\n\nNotes on \nfinishing\n and \nevaluating\n assemblies.", 
            "title": "Assembly and finishing with command line tools"
        }, 
        {
            "location": "/modules/cmdline_assembly/#pacbio-reads-assembly-with-command-line-tools", 
            "text": "Keywords: de novo assembly, PacBio, PacificBiosciences, Illumina, command line, Canu, Circlator, BWA, Spades, Pilon, Microbial Genomics Virtual Laboratory  This tutorial demonstrates how to use long Pacbio sequence reads to assemble a bacterial genome and plasmids, including correcting the assembly with short Illumina reads.", 
            "title": "Pacbio reads: assembly with command line tools"
        }, 
        {
            "location": "/modules/cmdline_assembly/#learning-objectives", 
            "text": "At the end of this tutorial, be able to use command line tools to produce a bacterial genome assembly using the following workflow:   get data  assemble long (Pacbio) reads  trim overhangs and circularise  search for smaller plasmids  correct with short (Illumina) reads", 
            "title": "Learning objectives"
        }, 
        {
            "location": "/modules/cmdline_assembly/#overview", 
            "text": "Simplified version of workflow:", 
            "title": "Overview"
        }, 
        {
            "location": "/modules/cmdline_assembly/#get-data", 
            "text": "Open the mGVL command line  Navigate to or create the directory in which you want to work.\ne.g.   mkdir staph\ncd staph  If you already have the files ready, skip forward to next section,  Assemble .", 
            "title": "Get data"
        }, 
        {
            "location": "/modules/cmdline_assembly/#find-the-pacbio-files-for-this-sample", 
            "text": "If the files are already on your server, you can symlink by using   ln -s real_file_path chosen_symlink_name   Alternatively, obtain the input files from elsewhere, e.g. from the BPA portal. (You will need a password.)  Pacbio files are often stored in the format: Sample_name/Cell_name/Analysis_Results/long_file_name_1.fastq.gz  We will use the  longfilename.subreads.fastq.gz  files.  The reads are usually split into three separate files because they are so large.  Right click on the first  subreads.fastq.gz  file and  copy link address .  In the command line, type:   wget --user username --password password [paste link URL for file]   Repeat for the other two  subreads.fastq.gz  files.", 
            "title": "Find the Pacbio files for this sample"
        }, 
        {
            "location": "/modules/cmdline_assembly/#join-pacbio-fastq-files", 
            "text": "If the files are gzipped, type:   cat filepath/filep0.*.subreads.fastq.gz   subreads.fastq.gz   If the files are not gzipped, type:   cat filepath/filep0.*.subreads.fastq | gzip   subreads.fastq.gz   We now have a file called  subreads.fastq.gz .", 
            "title": "Join Pacbio fastq files"
        }, 
        {
            "location": "/modules/cmdline_assembly/#find-the-illumina-files-for-this-sample", 
            "text": "We will also use 2 x Illumina (Miseq) fastq.gz files.  These are the  R1.fastq.gz  and  R2.fastq.gz  files.  Right click on the file name and  copy link address .  In the command line, type:   wget --user username --password password [paste link URL for file]   Repeat for the other read.fastq.gz file.  Shorten the name of each of these files:   mv longfilename_R1.fastq.gz R1.fastq.gz\nmv longfilename_R2.fastq.gz R2.fastq.gz", 
            "title": "Find the Illumina files for this sample"
        }, 
        {
            "location": "/modules/cmdline_assembly/#view-files", 
            "text": "Type  ls  to display the folder contents.   ls   The 3 files we will use in this analysis are:  subreads.fastq.gz  (the Pacbio reads)  R1.fastq.gz  and  R2.fastq.gz  (the Illumina reads)       In this tutorial we will use  Staphylococcus aureus  sample 25745.", 
            "title": "View files"
        }, 
        {
            "location": "/modules/cmdline_assembly/#assemble", 
            "text": "We will use the assembly software called  Canu .  Run Canu with these commands:   canu -p staph -d output genomeSize=2.8m -pacbio-raw subreads.fastq   staph  is the prefix given to output files  output  is the name of the output directory  genomeSize  only has to be approximate.  e.g.  Staphylococcus aureus , 2.8m  e.g.  Streptococcus pyogenes , 1.8m     the  reads  can be unzipped or .gz    Canu will correct, trim and assemble the reads.   This will take ~ 30 minutes.", 
            "title": "Assemble"
        }, 
        {
            "location": "/modules/cmdline_assembly/#check-the-output", 
            "text": "cd output   The  staph.contigs.fasta  are the assembled sequences.  The  staph.unassembled.fasta  are the reads that could not be assembled.  The  staph.correctedReads.fasta.gz  are the corrected Pacbio reads that were used in the assembly.  The  staph.file.gfa  is the graph of the assembly.  Display summary information about the contigs:   infoseq staph.contigs.fasta    (infoseq is a tool from  EMBOSS )    This will show the number of contigs, e.g.        - tig00000000   dna 2746242\n    - tig00000001   dna 48500", 
            "title": "Check the output"
        }, 
        {
            "location": "/modules/cmdline_assembly/#change-canu-parameters-if-required", 
            "text": "If the assembly is poor with many contigs, re-run Canu with extra sensitivity parameters; e.g.   canu -p prefix -d outdir corMhapSensitivity=high corMinCoverage=0 genomeSize=2.8m -pacbio-raw subreads.fastq", 
            "title": "Change Canu parameters if required"
        }, 
        {
            "location": "/modules/cmdline_assembly/#trim-and-circularise", 
            "text": "", 
            "title": "Trim and circularise"
        }, 
        {
            "location": "/modules/cmdline_assembly/#run-circlator", 
            "text": "Circlator identifies and trims overhangs (on chromosomes and plasmids) and orients the start position at an appropriate gene (e.g. dnaA). It takes in the assembled contigs from Canu, as well as the corrected reads prepared by Canu.  To run:  circlator all --threads 8 --verbose --b2r_length_cutoff 20000 ../staph.contigs.fasta ../staph.corrected.reads.fastq.gz circlator_all_output   threads  is the number of cores  verbose  prints progress information to the screen  b2r_length_cutoff  using approximately 2X average read length (could be omitted at first; if all contigs don t circularise, include this option to see if any improvement)  ../staph.contigs.fasta  is the file path to the input multi-fasta assembly  ../staph.corrected.reads.fastq.gz  is the file path to the corrected Pacbio reads  circlator_all_output  is the name of the output directory.   Check the output: were the contigs circularised:  less 04.merge.circularise.log  Where were the contigs oriented (which gene):  less 06.fixstart.log  What are the trimmed contig sizes:  fa -f 06.fixstart.fasta  The trimmed contigs are in the file called  06.fixstart.fasta . Re-name it  contig_1_2.fa :  mv 06.fixstart.fasta contig_1_2.fa", 
            "title": "Run Circlator"
        }, 
        {
            "location": "/modules/cmdline_assembly/#find-smaller-plasmids", 
            "text": "Pacbio reads are long, and may have been longer than small plasmids. We will look for any small plasmids using the Illumina reads.  This section involves several steps:   Use the multifasta canu-circlator output of trimmed assembly contigs.  Map all the Illumina reads against these Pacbio assembled contigs.  Extract any reads that  didn t  map and assemble them together: this could be a plasmid, or part of a plasmid.  Look for overhang: if found, trim. If not, continue:  Search Genbank for any matching proteins: a replication protein found.    Assemble all the Illumina reads and produce an assembly graph.  Search the graph for a match to the replication protein and its adjoining regions.  Extract this longer sequence from the Illumina assembly: this is the small plasmid.  Check for overhang in this plasmid and trim.", 
            "title": "Find smaller plasmids"
        }, 
        {
            "location": "/modules/cmdline_assembly/#align-illumina-with-bwa", 
            "text": "Align illumina reads to these contigs  First, index the contigs file   bwa index contig_1_2.fa   then, align using bwa mem   bwa mem -t 8 contig_1_2.fa R1.fastq.gz R2.fastq.gz | samtools sort   aln.bam   bwa mem  is the alignment tool  -t 8  is the number of cores  contig_1_2.fa  is the input assembly file  R1.fastq.gz R2.fastq.gz  are the Illumina reads   | samtools sort  pipes the output to samtools to sort   aln.bam  sends the alignment to the file  aln.bam", 
            "title": "Align Illumina with BWA"
        }, 
        {
            "location": "/modules/cmdline_assembly/#extract-unmapped-illumina-reads", 
            "text": "Index the alignment file   samtools index aln.bam   extract the fastq files from the bam alignment - those reads that were unmapped to the Pacbio alignment - and save them in various  unmapped  files:   samtools fastq -f 4 -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq aln.bam   fastq  is a command that coverts a  .bam  file into fastq format  -f 4  : only output unmapped reads  -1  : put R1 reads into a file called  unmapped.R1.fastq  -2  : put R2 reads into a file called  unmapped.R2.fastq  -s  : put singleton reads into a file called  unmapped.RS.fastq  aln.bam  : input alignment file   We now have three files of the unampped reads:    unmapped.R1.fastq   unmapped.R2.fastq   unmapped.RS.fastq", 
            "title": "Extract unmapped Illumina reads"
        }, 
        {
            "location": "/modules/cmdline_assembly/#assemble-the-unmapped-reads", 
            "text": "assemble with spades   spades.py -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq --careful --cov-cutoff auto -o spades_assembly   -1  is input file forward  -2  is input file reverse  -s  is unpaired   careful  : minimizes mismatches and short indels   cov-cutoff auto  : computes the coverage threshold (rather than the default setting,  off )  -o  is the output directory   cd spades_assembly\ninfoseq contigs.fasta   shows how many assembled:  e.g. no=135  max = 2229    sort fasta by size of seqs:   sizeseq\ninput sequence set: contigs.fasta\nreturn longest sequence first [N]: Y\noutput sequence(s) [contigs.fasta]: sorted_contigs.fasta  Print the first row of each seq to see coverage:  grep cov sorted_contigs.fasta     result: NODE_1_length_2229_cov_610.583  longest contig is 2229 and high coverage    all the nodes are listed  see if any other nodes have high coverage  e.g. NODE_135_length_78_cov_579    look at the sequence of this contig:   tail sorted_contigs.fasta   This is a homopolymer, so disregard.  We will extract the first sequence (NODE_1):   samtools faidx sorted_contigs.fasta\nsamtools faidx sorted_contigs.fasta NODE_1_length_2229_cov_610.583   contig3.fa   this is now saved as  contig3.fa  open this file in nano, make the header  contig3 , save.", 
            "title": "Assemble the unmapped reads"
        }, 
        {
            "location": "/modules/cmdline_assembly/#investigate-the-small-plasmid-contig3", 
            "text": "Blast the start of contig3 against itself  Take the start of the contig:   head -n 10 contig3.fa   contig3.fa.head   We want to see if it matches the end (overhang)  Format the assembly file for blast:   makeblastdb -in contig3.fa -dbtype nucl   blast the start of the assembly (.head file) against all of the assembly:   blastn -query contig3.fa.head -db contig3.fa -evalue 1e-3 -dust no -out contig3.bls   look at  contig3.bls  to see hits:   less contig3.bls   the first hit is against itself, as expected  there are no few further hits, so we assume there is no overhang that needs trimming.  however, the sequence is likely then to be longer than this.   less contig3.fa   Copy the sequence  Go to NCBI:  https://blast.ncbi.nlm.nih.gov/Blast.cgi ; choose blastx  Paste the sequence from contig3.fa  Choose genetic code = 11  Blast  This hits a replication (plasmid) protein. Hypothesise that   this is a small plasmid; search for the entire sequence within the assembly of all the Illumina reads (next step).", 
            "title": "Investigate the small plasmid (contig3)"
        }, 
        {
            "location": "/modules/cmdline_assembly/#assemble-all-the-illumina-reads", 
            "text": "Assemble all the Illumina reads with spades (not just those reads that did not map to the Pacbio assembly).   spades.py -1 R1.fastq -2 R2.fastq --careful --cov-cutoff auto -o spades_assembly_all_illumina   Navigate to the output:  cd spades_assembly_all_illumina   in here is the  assembly_graph.fastg  Transfer this file to your local computer (e.g. using the file transfer program  Cyberduck ).  Examine the assembly in the program  Bandage .  File: Load graph:  assembly_graph.fastg  In the left hand panel, click  Draw graph  Your assembly graph may look like this:     Blast the small plasmid sequence in this assembly   In the left hand panel: Blast: create/view BLAST search  Build blast database  Paste in the sequence of contig3  Blast  The main hit is around node 10.     Go to the main Bandage window   find nodes  in right hand panel - 10  This node is slightly longer: 2373: this could be the plasmid  Extract this node in fasta format: In the top panel, go to Output: Save selected node sequences; save as  contig3b.fa  Open this file in nano and change the header to  contig3b , save.", 
            "title": "Assemble all the illumina reads"
        }, 
        {
            "location": "/modules/cmdline_assembly/#trim-small-plasmid", 
            "text": "Take the start of the sequence and see if it matches the end:   head -n 10 contig3b.fa   contig3b.fa.head\nmakeblastdb -in contig3b.fa -dbtype nucl\nblastn -query contig3b.fa.head -db contig3b.fa -evalue 1e-3 -dust no -out contig3b.bls\nless contig3b.bls   The first hit is against the start of the chromosome, as expected.  The last hit starts at position 2253; we will trim the plasmid to position 2252  Index the contig3b.fa file:   samtools faidx contig3b.fa   Trim:   samtools faidx contig3b.fa contig3b:1-2252   contig3b.fa.trimmed   Open this file in nano and change the header to  contig3b , save.  We now have a trimmed contig3b.", 
            "title": "Trim small plasmid"
        }, 
        {
            "location": "/modules/cmdline_assembly/#collect-all-contigs-in-one-file", 
            "text": "cat contig_1_2.fa contig3b.fa.trimmed   all_contigs.fa   See the three contigs and sizes:   infoseq all_contigs.fa", 
            "title": "Collect all contigs in one file"
        }, 
        {
            "location": "/modules/cmdline_assembly/#correct", 
            "text": "We will correct the Pacbio assembly, first with Pacbio corrected reads (from Canu) and then with Illumina reads.   inputs:  Draft Pacbio assembly (overhang trimmed from each of the three replicons)  corrected (and trimmed) Pacbio reads from Canu.  illumina reads (aligned to Pacbio assembly: in bam format)    output: corrected assembly", 
            "title": "Correct"
        }, 
        {
            "location": "/modules/cmdline_assembly/#1-correct-with-pacbio-correctedtrimmed-reads", 
            "text": "Align reads to assembly:  bwa index all_contigs.fa\nbwa mem -t 32 all_contigs.fa canu.trimmedReads.fasta.gz | samtools sort   aln.bam\nsamtools index aln.bam\nsamtools faidx all_contigs.fa   -t  is the number of cores (e.g. 8)  to find out how many you have, grep -c processor /proc/cpuinfo    now we have an alignment file to use in pilon:  aln.bam   Run pilon:  pilon --genome all_contigs.fa --unpaired aln.bam --output pilon1 --fix all --mindepth 0.5 --changes --verbose   genome  is the name of the input assembly to be corrected  unpaired  is the alignment of the reads against the assembly (use  unpaired  instead of  frags  where alignment is from pacbio reads)  output  is the name of the output prefix  fix  is an option for types of corrections  mindepth  gives a minimum read depth to use  changes  produces an output file of the changes made  verbose  prints information to the screen during the run  if you are using pilon on a different machine and you want to specify the number of CPUs, type in  threads  number (e.g. 32).   Look at the .changes file:  less pilon1.changes | column -t   This shows the corrections made by Pilon:     Look at the details of the .fasta file:   infoseq pilon1.fasta", 
            "title": "1. Correct with Pacbio corrected/trimmed reads"
        }, 
        {
            "location": "/modules/cmdline_assembly/#2-correct-with-illumina-reads", 
            "text": "Input: the corrected assembly from the previous step:  pilon1.fasta  Align the Illumina reads to this assembly:  bwa index pilon1.fasta\nbwa mem -t 32 pilon1.fasta R1.fastq.gz R2.fastq.gz | samtools sort   aln.bam\nsamtools index aln.bam\nsamtools faidx pilon1.fasta  Look at how the illumina reads are aligned:  samtools tview -p contig1 aln.bam pilon1.fasta  note:  contig1  is the name of the contig to view; e.g. tig00000000.  Run pilon (note: use  frags  this time instead of  unpaired ):  pilon --genome pilon1.fa --frags aln.bam --output pilon2 --fix all --mindepth 0.5 --changes --verbose  Look at the changes file:  less pilon2.changes  Look at the .fasta file:  less pilon2.fasta  If there are more than 2 changes, run Pilon again, using the pilon2.fasta file as the input assembly, and the Illumina reads to correct.  Final output:   the corrected genome assembly of  Staphylococcus aureus  in .fasta format, containing three contigs: chromosome, large plasmid and small plasmid.", 
            "title": "2. Correct with Illumina reads"
        }, 
        {
            "location": "/modules/cmdline_assembly/#next", 
            "text": "Further analyses:   Annotate with Prokka.  Comparative genomics, e.g. with Roary.   Links:   Details of bas.h5 files  Canu  manual  and  gitub repository  Circlator  article  and  github repository  Pilon  article  and  github repository  Notes on  finishing  and  evaluating  assemblies.", 
            "title": "Next"
        }, 
        {
            "location": "/modules/dge/", 
            "text": "Differential Gene Expression\n\n\nKeywords: differential gene expression, DGE, RNA, RNA-Seq, transcriptomics, Degust, voom, limma, Galaxy, Microbial Genomics Virtual Laboratory. \n\n\nThis tutorial is about differential gene expression in bacteria, using Galaxy tools and Degust (web).\n\n\n\n\n\nBackground\n\n\nDifferential Gene Expression (DGE) is the process of determining whether any genes were expressed at a different level between two conditions. For example, the conditions could be wildtype versus mutant, or two growth conditions. Usually multiple biological replicates are done for each condition - these are needed to separate variation within the condition from that between the conditions.\n\n\nLearning Objectives\n\n\nAt the end of this tutorial you should be able to:\n\n\n\n\nAlign RNA-Seq data to a reference genome  \n\n\nCount transcripts for each sample\n\n\nPerform statistical analysis to obtain a list of differentially expressed genes\n\n\nVisualize and interpret the results\n\n\n\n\nInput data: reads and reference\n\n\nRNA-Seq reads\n\n\nA typical experiment will have 2 conditions each with 3 replicates, for a total of 6 samples.\n\n\n\n\nOur RNA-seq reads are from 6 samples in \nFASTQ\n format.\n\n\nWe have single-end reads; so one file per sample.\n\n\nData could also be paired-end reads, and there would be two files per sample.\n\n\n\n\n\n\nThese have been reduced to 1% of their original size for this tutorial.\n\n\nThe experiment used the bacteria \nE. coli\n grown in two conditions.\n\n\nFiles labelled \nLB\n are the wildtype\n\n\nFiles labelled \nMG\n have been exposed to 0.5% \nMG - alpha methyglucoside (a sugar solution).\n\n\n\n\n\n\n\n\n\n\n\nReference genome\n\n\nThe reference genomes is in \nFASTA\n format and the gene annotations are in \nGTF\n format.\n\n\n\n\nThe \nFASTA\n file contains the DNA sequence(s) that make up the genome; e.g. the chromosome and any plasmids.\n\n\nThe \nGTF\n file lists the coordinates (position) of each feature. Commonly-annotated features are genes, transcripts and protein-coding sequences.\n\n\n\n\n\n\n\nUpload files to Galaxy\n\n\n\n\nLog in to your Galaxy server.\n\n\nIn the \nHistory\n pane, click on the cog\nicon, and select \nImport from File\n (at the bottom of the list).\n\n\nUnder \nArchived History URL\n paste:\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/Galaxy-History-BacterialDGE.tar.gz\n\n\nIn the \nHistory\n pane, click on the view\nicon and find the uploaded history.\n\n\n(This may take a minute. Refresh the page.)\n\n\n\n\n\n\nClick \nSwitch to\n that history, then \nDone\n.\n\n\nThe files should now be ready to use in your current History pane.\n\n\n\n\n\n\nAlign reads to reference\n\n\nThe RNA-Seq reads are fragmented and are not complete transcripts. To determine the transcripts from which the reads originated (and therefore, to which gene they correspond) we can map them to a reference genome.\n\n\nIn Galaxy:\n\n\n\n\nGo to \nTools \n NGS Analysis \n NGS: Mapping \n Map with BWA-MEM\n\n\nUnder \nWill you select a reference genome from your history or use a built-in index?\n: \nUse a genome from history and build index\n\n\nUse the following dataset as the reference sequence\n: \nEcoli_k12.fasta\n\n\nSingle or Paired-end reads\n: \nsingle\n\n\nSelect fastq dataset\n:\n\n\nClick on the \nMultiple Datasets\n icon in centre\n\n\nSelect all 6 \nFASTQ\n files (they turn blue; use side-scroll bar to check all have been selected)\n\n\nThis will map each set of reads to the reference genome\n\n\n\n\n\n\n\n\nYour tool interface should look like this:\n\n\n\n\n\n\nClick \nExecute\n\n\nClick \nRefresh\n in the history pane to see if the analysis has finished.\n\n\n\n\nOutput: 6 \nbam\n files of reads mapped to the reference genome.\n\n\n\n\n\n\nRe-name the output files:\n\n\n\n\nThese are called \nMap with BWA-MEM on data x and data x\n.\n\n\nClick on the pencil icon next to each of these and re-name them as their sample name (e.g. LB1, LB2 etc.).\n\n\nClick \nSave\n.\n\n\n\n\n\n\n\n\nCount reads per gene\n\n\nWe now need to count how many reads overlap with particular genes. The information about gene names is from the annotations in the GTF file.\n\n\nIn Galaxy:\n\n\n\n\nGo to \nTools \n NGS Analysis \n NGS: RNA Analysis \n SAM/BAM to count matrix\n.\n\n\nNote: Don\nt select the tool called \nhtseq-count\n. The \nSAM/BAM to count matrix\n also uses that tool but allows an input of multiple bam files, which is what we want.\n\n\n\n\n\n\nFor \nGene model (GFF) file to count reads over from your current history\n, select the \nGTF\n file.\n\n\nFor \nReads are stranded\n select \nYes\n (box turns dark grey)\n\n\nFor \nGTF feature type for counting reads\n select \ntranscript\n.\n\n\nFor \nbam/sam file from your history\n choose the 6 \nbam\n files.\n\n\n\n\nYour tool interface should look like this:\n\n\n\n\n\n\nClick \nExecute\n\n\nClick \nRefresh\n in the history pane to see if the analysis has finished.\n\n\n\n\nOutput:\n\n\n\n\nThere is one output file: \nbams to DGE count matrix\n.\n\n\nClick on the file name to expand the information in the History pane.\n\n\nClick on the file \nicon underneath to download it to your computer for use later on in this tutorial.\n\n\nClick on the eye icon to see this file.\n\n\n\n\n\n\n\n\nEach row is a gene (or feature) and each column is a sample, with counts against each gene.\n\n\nHave a look at how the counts vary between samples, per gene.\n\n\nWe can\nt just compare the counts directly; they need to be normalized before comparison, and this will be done as part of the DGE analysis in the next step.\n\n\n\n\nDGE in Degust\n\n\nDegust is a tool on the web that can analyse the counts files produced in the step above, to test for differential gene expression.\n\n\n(Degust can also display the results from DGE analyses performed elsewhere.)\n\n\nUpload counts file\n\n\nGo to the \nDegust web page\n. Click \nGet Started\n.\n\n\n\n\n\n\nClick on \nChoose File\n.\n\n\nSelect the \nhtseq output file. tabular\n (that you previously downloaded to your computer from Galaxy) and click \nOpen\n.\n\n\nClick \nUpload\n.\n\n\n\n\nA Configuation page will appear.\n\n\n\n\nFor \nName\n type \nDGE in E coli\n\n\nFor \nInfo columns\n select \nContig\n\n\nFor \nAnalyze server side\n leave box checked.\n\n\nFor \nMin read count\n put \n10\n.\n\n\nClick \nAdd condition\n\n\nAdd a condition called \nControl\n and select the LB columns.\n\n\nAdd a condition called \nTreament\n and select the MG columns.\n\n\n\n\n\n\n\n\nYour Configuration page should look like this:\n\n\n\n\n\n\nSave changes\n\n\nView\n - this brings up the Degust viewing window.\n\n\n\n\nOverview of Degust sections\n\n\n\n\nTop black panel with \nConfigure\n settings at right.\n\n\nLeft: Conditions: Control and Treatment.\n\n\nLeft: Method selection for DGE.\n\n\nTop centre: Plots, with options at right.\n\n\nWhen either of the expression plots are selected, a heatmap appears below.\n\n\nA table of genes (or features); expression in treatment relative to control (Treatment column); and significance (FDR column).  \n\n\n\n\n\n\nAnalyze gene expression\n\n\n\n\nUnder \nMethod\n, make sure that \nVoom/Limma\n is selected.\n\n\nClick \nApply\n. This runs Voom/Limma on the uploaded counts.\n\n\n\n\nMDS plot\n\n\nFirst, look at the MDS plot.\n\n\n\n\n\n\nThis is a multidimensional scaling plot which represents the variation between samples.\n\n\nIdeally:\n\n\nAll the LB samples would be close to each other\n\n\nAll the MG samples would be close to each other\n\n\nThe LB and MG groups would be far apart\n\n\n\n\n\n\nThe x-axis is the dimension with the highest magnitude. The control/treatment samples should be split along this axis.\n\n\nOur LB samples are on the left and the MG samples are on the right, which means they are well separated on their major MDS dimension, which looks correct.\n\n\n\n\nExpression - MA plot\n\n\nEach dot shows the change in expression in one gene.\n\n\n\n\nThe average expression (over both condition and treatment samples) is represented on the x-axis.\n\n\nPlot points should be symmetrical around the x-axis.\n\n\nWe can see that many genes are expressed at a low level, and some are highly expressed.\n\n\n\n\n\n\nThe fold change is represented on the y axis.\n\n\nIf expression is significantly different between treatment and control, the dots are red. If not, they are blue. (In Degust, significant means FDR \n0.05).\n\n\nAt low levels of gene expression (low values of the x axis), fold changes are less likely to be significant.\n\n\n\n\n\n\n\n\nClick on the dot to see the gene name.     \n\n\n\n\nExpression - Parallel Coordinates and heatmap\n\n\nEach line shows the change in expression in one gene, between control and treatment.\n\n\n\n\nGo to \nOptions\n at the right.\n\n\nFor \nFDR cut-off\n set at 0.001.\n\n\nThis is a significance level (an adjusted p value). We will set it quite low in this example, to ensure we only examine key differences.\n\n\n\n\n\n\n\n\nLook at the Parallel Coordinates plot. There are two axes:\n\n\n\n\nLeft: \nControl\n: Gene expression in the control samples. All values are set at zero.\n\n\nRight: \nTreatment\n Gene expression in the treatment samples, relative to expression in the control.\n\n\n\n\n\n\n\n\nThe blocks of blue and red underneath the plot are called a heatmap.\n\n\n\n\nEach block is a gene. Click on a block to see its line in the plot above.\n\n\nLook at the row for the Treatment. Relative to the control, genes expressed more are red; genes expressed less are blue.\n\n\n\n\n\n\n\n\n\n\nNote:\n\n\n\n\nfor an experiment with multiple treatments, the various treatment axes can be dragged to rearrange. There is no natural order (such as a time series).\n\n\n\n\nTable of genes\n\n\n\n\nContig\n: names of genes. Note that gene names are sometimes specific to a species, or they may be only named as a locus ID (a chromosomal location specified in the genome annotation).\n\n\nFDR\n: False Discovery Rate. This is an adjusted p value to show the significance of the difference in gene expression between two conditions. Click on column headings to sort. By default, this table is sorted by FDR.\n\n\nControl\n and \nTreatment\n: log2(Fold Change) of gene expression. The default display is of fold change in the treatment relative to the control. Therefore, values in the \nControl\n column are zero. This can be changed in the \nOptions\n panel at the top right.\n\n\nIn some cases, a large fold change will be meaningful but in others, even a small fold change can be important biologically.\n\n\n\n\nTable of genes and expression:\n\n\n\n\n\n\n\nDGE in Galaxy\n\n\nDifferential gene expression can also be analyzed in Galaxy. The input is the count matrix produced by a tool such as HTSeq-Count (see section above: \nCount reads per gene\n).\n\n\n\n\nGo to \nTools \n NGS Analysis \n NGS: RNA Analysis \n Differential Count models\n\n\nThis has options to use edgeR, DESeq, or Voom. Here we will use Voom.\n\n\n\n\n\n\nFor \nSelect an input matrix\n choose the \ncount matrix\n file generated in the previous step.\n\n\nFor \nTitle for job outputs\n enter \nDGE using voom\n.\n\n\nFor \nSelect columns containing treatment\n tick boxes for the MG samples.\n\n\nFor \nSelect columns containing control\n tick boxes for the LB samples.\n\n\nUnder \nRun this model using edgeR\n choose \nDo not run edgeR\n.\n\n\nUnder \nRun the same model with DESeq2 and compare findings\n choose \nDo not run DESeq2\n.\n\n\nUnder \nRun the same model with Voom/limma and compare findings\n choose \nRun VOOM\n.\n\n\n\n\nYour tool interface should look like this:\n\n\n\n\n\n\nClick \nExecute\n.\n\n\n\n\nThere are two output files.\n\n\nView the file called \nDGEusingvoom.html\n.\n\n\n\n\nScroll down to \nVOOM log output\n and \n#VOOM top 50\n.\n\n\nThe \nContig\n column has the gene names.\n\n\nLook at the \nadj.P.Val\n column. This is an adjusted p value to show the significance of the gene expression difference, accounting for the effect of multiple testing. Also known as False Discovery Rate. The table is ordered by the values in this column.\n\n\nLook at the \nlogFC\n column. This is log2(Fold Change) of relative gene expression between the treatment samples and the control samples.\n\n\n\n\nView the file called \nDEGusingvoom_topTable_VOOM.xls\n.\n\n\n\n\nThis is a list of all the genes that had transcripts mapped, and associated statistics.\n\n\n\n\nWhat next?\n\n\nTo learn more about the differentially-expressed genes:\n\n\n\n\nGo to \nthe NCBI website.\n\n\nUnder \nAll Databases\n, click on \nGene\n\n\nEnter the gene name in the search bar; e.g. ptsG\n\n\nClick on the first result that matches the species (e.g. in this case, \nE. coli\n).\n\n\nThis provides information about the gene, and may also show further references (e.g. in this case, a link to the EcoGene resource).\n\n\n\n\n\n\n\n\nSome of the most (statistically) significant differentially-expressed genes in this experiment are:\n\n\n\n\nptsG\n: a glucose-specific transporter.\n\n\nsetA\n: a sugar efflux transporter; is induced by glucose-phosphate stress.\n\n\nsucD\n: the alpha subunit of the the gene for succinyl-CoA synthetase; involved in ATP production.\n\n\nsucB\n: a component of the 2-oxoglutarate dehydrogenase complex; catalyzes a step in the Krebs cycle.\n\n\ndeoC\n: 2-deoxyribose-5-phosphate aldolase; binds selenium; may be involved in selenium transport.\n\n\n\n\nNext steps: Investigate the biochemical pathways involving the genes of interest.\n\n\nMore information\n\n\n\n\nLink to Degust.\n\n\nLink to Voom paper.", 
            "title": "Differential gene expression"
        }, 
        {
            "location": "/modules/dge/#differential-gene-expression", 
            "text": "Keywords: differential gene expression, DGE, RNA, RNA-Seq, transcriptomics, Degust, voom, limma, Galaxy, Microbial Genomics Virtual Laboratory.   This tutorial is about differential gene expression in bacteria, using Galaxy tools and Degust (web).", 
            "title": "Differential Gene Expression"
        }, 
        {
            "location": "/modules/dge/#background", 
            "text": "Differential Gene Expression (DGE) is the process of determining whether any genes were expressed at a different level between two conditions. For example, the conditions could be wildtype versus mutant, or two growth conditions. Usually multiple biological replicates are done for each condition - these are needed to separate variation within the condition from that between the conditions.", 
            "title": "Background"
        }, 
        {
            "location": "/modules/dge/#learning-objectives", 
            "text": "At the end of this tutorial you should be able to:   Align RNA-Seq data to a reference genome    Count transcripts for each sample  Perform statistical analysis to obtain a list of differentially expressed genes  Visualize and interpret the results", 
            "title": "Learning Objectives"
        }, 
        {
            "location": "/modules/dge/#input-data-reads-and-reference", 
            "text": "RNA-Seq reads  A typical experiment will have 2 conditions each with 3 replicates, for a total of 6 samples.   Our RNA-seq reads are from 6 samples in  FASTQ  format.  We have single-end reads; so one file per sample.  Data could also be paired-end reads, and there would be two files per sample.    These have been reduced to 1% of their original size for this tutorial.  The experiment used the bacteria  E. coli  grown in two conditions.  Files labelled  LB  are the wildtype  Files labelled  MG  have been exposed to 0.5%  MG - alpha methyglucoside (a sugar solution).      Reference genome  The reference genomes is in  FASTA  format and the gene annotations are in  GTF  format.   The  FASTA  file contains the DNA sequence(s) that make up the genome; e.g. the chromosome and any plasmids.  The  GTF  file lists the coordinates (position) of each feature. Commonly-annotated features are genes, transcripts and protein-coding sequences.    Upload files to Galaxy   Log in to your Galaxy server.  In the  History  pane, click on the cog icon, and select  Import from File  (at the bottom of the list).  Under  Archived History URL  paste: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Microbial_tutorials/Galaxy-History-BacterialDGE.tar.gz  In the  History  pane, click on the view icon and find the uploaded history.  (This may take a minute. Refresh the page.)    Click  Switch to  that history, then  Done .  The files should now be ready to use in your current History pane.", 
            "title": "Input data: reads and reference"
        }, 
        {
            "location": "/modules/dge/#align-reads-to-reference", 
            "text": "The RNA-Seq reads are fragmented and are not complete transcripts. To determine the transcripts from which the reads originated (and therefore, to which gene they correspond) we can map them to a reference genome.  In Galaxy:   Go to  Tools   NGS Analysis   NGS: Mapping   Map with BWA-MEM  Under  Will you select a reference genome from your history or use a built-in index? :  Use a genome from history and build index  Use the following dataset as the reference sequence :  Ecoli_k12.fasta  Single or Paired-end reads :  single  Select fastq dataset :  Click on the  Multiple Datasets  icon in centre  Select all 6  FASTQ  files (they turn blue; use side-scroll bar to check all have been selected)  This will map each set of reads to the reference genome     Your tool interface should look like this:    Click  Execute  Click  Refresh  in the history pane to see if the analysis has finished.   Output: 6  bam  files of reads mapped to the reference genome.    Re-name the output files:   These are called  Map with BWA-MEM on data x and data x .  Click on the pencil icon next to each of these and re-name them as their sample name (e.g. LB1, LB2 etc.).  Click  Save .", 
            "title": "Align reads to reference"
        }, 
        {
            "location": "/modules/dge/#count-reads-per-gene", 
            "text": "We now need to count how many reads overlap with particular genes. The information about gene names is from the annotations in the GTF file.  In Galaxy:   Go to  Tools   NGS Analysis   NGS: RNA Analysis   SAM/BAM to count matrix .  Note: Don t select the tool called  htseq-count . The  SAM/BAM to count matrix  also uses that tool but allows an input of multiple bam files, which is what we want.    For  Gene model (GFF) file to count reads over from your current history , select the  GTF  file.  For  Reads are stranded  select  Yes  (box turns dark grey)  For  GTF feature type for counting reads  select  transcript .  For  bam/sam file from your history  choose the 6  bam  files.   Your tool interface should look like this:    Click  Execute  Click  Refresh  in the history pane to see if the analysis has finished.   Output:   There is one output file:  bams to DGE count matrix .  Click on the file name to expand the information in the History pane.  Click on the file  icon underneath to download it to your computer for use later on in this tutorial.  Click on the eye icon to see this file.     Each row is a gene (or feature) and each column is a sample, with counts against each gene.  Have a look at how the counts vary between samples, per gene.  We can t just compare the counts directly; they need to be normalized before comparison, and this will be done as part of the DGE analysis in the next step.", 
            "title": "Count reads per gene"
        }, 
        {
            "location": "/modules/dge/#dge-in-degust", 
            "text": "Degust is a tool on the web that can analyse the counts files produced in the step above, to test for differential gene expression.  (Degust can also display the results from DGE analyses performed elsewhere.)", 
            "title": "DGE in Degust"
        }, 
        {
            "location": "/modules/dge/#upload-counts-file", 
            "text": "Go to the  Degust web page . Click  Get Started .    Click on  Choose File .  Select the  htseq output file. tabular  (that you previously downloaded to your computer from Galaxy) and click  Open .  Click  Upload .   A Configuation page will appear.   For  Name  type  DGE in E coli  For  Info columns  select  Contig  For  Analyze server side  leave box checked.  For  Min read count  put  10 .  Click  Add condition  Add a condition called  Control  and select the LB columns.  Add a condition called  Treament  and select the MG columns.     Your Configuration page should look like this:    Save changes  View  - this brings up the Degust viewing window.", 
            "title": "Upload counts file"
        }, 
        {
            "location": "/modules/dge/#overview-of-degust-sections", 
            "text": "Top black panel with  Configure  settings at right.  Left: Conditions: Control and Treatment.  Left: Method selection for DGE.  Top centre: Plots, with options at right.  When either of the expression plots are selected, a heatmap appears below.  A table of genes (or features); expression in treatment relative to control (Treatment column); and significance (FDR column).", 
            "title": "Overview of Degust sections"
        }, 
        {
            "location": "/modules/dge/#analyze-gene-expression", 
            "text": "Under  Method , make sure that  Voom/Limma  is selected.  Click  Apply . This runs Voom/Limma on the uploaded counts.", 
            "title": "Analyze gene expression"
        }, 
        {
            "location": "/modules/dge/#mds-plot", 
            "text": "First, look at the MDS plot.    This is a multidimensional scaling plot which represents the variation between samples.  Ideally:  All the LB samples would be close to each other  All the MG samples would be close to each other  The LB and MG groups would be far apart    The x-axis is the dimension with the highest magnitude. The control/treatment samples should be split along this axis.  Our LB samples are on the left and the MG samples are on the right, which means they are well separated on their major MDS dimension, which looks correct.", 
            "title": "MDS plot"
        }, 
        {
            "location": "/modules/dge/#expression-ma-plot", 
            "text": "Each dot shows the change in expression in one gene.   The average expression (over both condition and treatment samples) is represented on the x-axis.  Plot points should be symmetrical around the x-axis.  We can see that many genes are expressed at a low level, and some are highly expressed.    The fold change is represented on the y axis.  If expression is significantly different between treatment and control, the dots are red. If not, they are blue. (In Degust, significant means FDR  0.05).  At low levels of gene expression (low values of the x axis), fold changes are less likely to be significant.     Click on the dot to see the gene name.", 
            "title": "Expression - MA plot"
        }, 
        {
            "location": "/modules/dge/#expression-parallel-coordinates-and-heatmap", 
            "text": "Each line shows the change in expression in one gene, between control and treatment.   Go to  Options  at the right.  For  FDR cut-off  set at 0.001.  This is a significance level (an adjusted p value). We will set it quite low in this example, to ensure we only examine key differences.     Look at the Parallel Coordinates plot. There are two axes:   Left:  Control : Gene expression in the control samples. All values are set at zero.  Right:  Treatment  Gene expression in the treatment samples, relative to expression in the control.     The blocks of blue and red underneath the plot are called a heatmap.   Each block is a gene. Click on a block to see its line in the plot above.  Look at the row for the Treatment. Relative to the control, genes expressed more are red; genes expressed less are blue.      Note:   for an experiment with multiple treatments, the various treatment axes can be dragged to rearrange. There is no natural order (such as a time series).", 
            "title": "Expression - Parallel Coordinates and heatmap"
        }, 
        {
            "location": "/modules/dge/#table-of-genes", 
            "text": "Contig : names of genes. Note that gene names are sometimes specific to a species, or they may be only named as a locus ID (a chromosomal location specified in the genome annotation).  FDR : False Discovery Rate. This is an adjusted p value to show the significance of the difference in gene expression between two conditions. Click on column headings to sort. By default, this table is sorted by FDR.  Control  and  Treatment : log2(Fold Change) of gene expression. The default display is of fold change in the treatment relative to the control. Therefore, values in the  Control  column are zero. This can be changed in the  Options  panel at the top right.  In some cases, a large fold change will be meaningful but in others, even a small fold change can be important biologically.   Table of genes and expression:", 
            "title": "Table of genes"
        }, 
        {
            "location": "/modules/dge/#dge-in-galaxy", 
            "text": "Differential gene expression can also be analyzed in Galaxy. The input is the count matrix produced by a tool such as HTSeq-Count (see section above:  Count reads per gene ).   Go to  Tools   NGS Analysis   NGS: RNA Analysis   Differential Count models  This has options to use edgeR, DESeq, or Voom. Here we will use Voom.    For  Select an input matrix  choose the  count matrix  file generated in the previous step.  For  Title for job outputs  enter  DGE using voom .  For  Select columns containing treatment  tick boxes for the MG samples.  For  Select columns containing control  tick boxes for the LB samples.  Under  Run this model using edgeR  choose  Do not run edgeR .  Under  Run the same model with DESeq2 and compare findings  choose  Do not run DESeq2 .  Under  Run the same model with Voom/limma and compare findings  choose  Run VOOM .   Your tool interface should look like this:    Click  Execute .   There are two output files.  View the file called  DGEusingvoom.html .   Scroll down to  VOOM log output  and  #VOOM top 50 .  The  Contig  column has the gene names.  Look at the  adj.P.Val  column. This is an adjusted p value to show the significance of the gene expression difference, accounting for the effect of multiple testing. Also known as False Discovery Rate. The table is ordered by the values in this column.  Look at the  logFC  column. This is log2(Fold Change) of relative gene expression between the treatment samples and the control samples.   View the file called  DEGusingvoom_topTable_VOOM.xls .   This is a list of all the genes that had transcripts mapped, and associated statistics.", 
            "title": "DGE in Galaxy"
        }, 
        {
            "location": "/modules/dge/#what-next", 
            "text": "To learn more about the differentially-expressed genes:   Go to  the NCBI website.  Under  All Databases , click on  Gene  Enter the gene name in the search bar; e.g. ptsG  Click on the first result that matches the species (e.g. in this case,  E. coli ).  This provides information about the gene, and may also show further references (e.g. in this case, a link to the EcoGene resource).     Some of the most (statistically) significant differentially-expressed genes in this experiment are:   ptsG : a glucose-specific transporter.  setA : a sugar efflux transporter; is induced by glucose-phosphate stress.  sucD : the alpha subunit of the the gene for succinyl-CoA synthetase; involved in ATP production.  sucB : a component of the 2-oxoglutarate dehydrogenase complex; catalyzes a step in the Krebs cycle.  deoC : 2-deoxyribose-5-phosphate aldolase; binds selenium; may be involved in selenium transport.   Next steps: Investigate the biochemical pathways involving the genes of interest.", 
            "title": "What next?"
        }, 
        {
            "location": "/modules/dge/#more-information", 
            "text": "Link to Degust.  Link to Voom paper.", 
            "title": "More information"
        }, 
        {
            "location": "/modules/xtandem/", 
            "text": "Protein indentification using X!Tandem\n\n\nIntroduction\n\n\nThe high-throughput nature of proteomics mass spectrometry is enabled by a productive combination of data acquisition protocols and the computational tools used to interpret the resulting spectra (list of peaks). One of the key components in mainstream protocols is the generation of tandem mass (MS/MS) spectra by peptide fragmentation. This approach is currently used in the large majority of proteomics experiments to routinely identify hundreds to thousands of proteins from single mass spectrometry runs. In order to do that multiple tools have to be employed: \nLC-MS/MS\n to segregate components of proteomic samples associated with \nprotein identification\n (see \nFigure 1\n) softwares, \nX!Tandem\n[^xtand], \nMascot\n or \nSEQUEST\n all of which perform protein identification but with different algorithms.\n\n\n\n\n\n\nFigure 1\n \n \nGeneral overview of the experimental steps and flow of data in protein identification using shotgun proteomics experiment.\n[^figure1]. \n\n\n\n\nFigure 1\n shows a general overview of the experimental steps in protein identification. Sample proteins are first cleaved into peptides, which are then separated using chromatography (e.g. HPLC). The peptides are then ionised and then selected ions are fragmented to produce signature tandem mass spectrometry (MS/MS) spectra. Peptides are identified from the MS/MS spectra by using programs such as X!Tandem, which search against a database of known peptides. Sequences of the identified peptides are used to infer which proteins are present in the original sample.\n\n\nBackground\n\n\nLC-MS/MS Analysis\n\n\nLiquid Chromatography\n (LC) is a technique used to separate molecules in the solvent (mobile phase). Nowadays liquid chromatography utilising high pressure to force the solvent through the columns packed with porous particles (stationary phase) is referred as High Pressure Liquid Chromatography (HPLC) \n see \nFigure 2\n. After purifying the proteomic cell content, LC is able to separate the different proteins which are injected in the mass spectrometer. Each peak from the results will be analysed by mass spectrometry.\n\n\n\n\n\n\nFigure 2\n \n \nSchema of High Pressure Liquid Chromatography (HPLC)[^figure3].\n Compounds of the mixture are separated in the HPLC column according to various parameters (polarity, charge, affinity etc). The type of separation depends on the column being used). A detector flow cell is used to detect the separated compounds band.\n\n\n\n\nMass spectrometry\n (MS) \n see \nFigure 3\n \n has been widely used to analyse biological samples and has evolved into an indispensable tool for proteomics research. Fundamentally, MS measures the mass-to-charge ratio (m/z) of gas-phase ions. Mass spectrometers consist of an ion source that converts analyte molecules into gas-phase ions, a mass analyser that separates ionised analytes on the basis of m/z ratio and a detector that records the number of ions at each m/z value. \n\n\n\n\n\n\nFigure 3\n \n \nSchema of mass specter\n. \nA mass spectrometer consists of three components: an ion source, a mass analyzer, and a detector. The ionizer converts a portion of the sample into ions. There is a wide variety of ionization techniques, depending on the phase (solid, liquid, gas) of the sample and the efficiency of various ionization mechanisms for the unknown species. An extraction system removes ions from the sample, which are then targeted through the mass analyzer and onto the detector. The differences in masses of the fragments allows the mass analyzer to sort the ions by their mass-to-charge ratio. The detector measures the value of an indicator quantity and thus provides data for calculating the abundances of each ion present.\n [^figure4]\n\n\n\n\nTandem mass spectrometry\n (MS/MS) \n see \nFigure 4\n \n is a key technique for protein or peptide sequencing and post-translational modifications analysis. Collision-induced dissociation (CID) has been the most widely used MS/MS technique in proteomics research. In this method, gas-phase peptide/protein cations are internally heated by multiple collisions with rare gas atoms. The result is fragmented ions that, after the detection phase and reconstitution, reveal the amino-acid chains. \n\n\n\n\n\n\nFigure 4\n \n \nSchema of a tandem mass spectrometry associated with liquid chromatography (LC-MS/MS)\n. This technique combine separation form liquid chromatography and m/z ratio analyse of the tandem mass spectrometry in order to generate spectra.\n\n\n\n\nFile formats\n : During a full proteomics analysis, as seen in \nFigure 5\n, many files are created. Every step has it\ns own multiples files formats :\n\n\n\n\n\n\nFigure 5\n \n \nMultiple formats during MS treatment\n[^MS file format]. From the sample processing to the final file, various file formats can be used. This schema shows the different processes and file formats that are generated during a full MS experiment.\n\n\n\n\nFor this tutorial we will focus on the \nInformatics Analysis\n part using the following file formats:\n\n\n\n\nfasta\n: fasta files are mainly used for representing either nucleotide sequences or peptide sequences, using single-letter codes.\n\n\nMGF\n : Mascot Generic Format (MGF) file is the most common format for MS/MS data encoding in the form of a peak list[^MGF]. This file encodes multiple MS/MS spectra in a single file listing the [m/z, intensity] pairs separated by headers \n see \nFigure 6\n. More precisely each query represents a complete MS/MS spectrum delimited by BEGIN IONS and END IONS statements. Embedded parameters[^embedded_parameters] can be found after each BEGIN IONS statement. An example entry is shown in the figure below:\n\n\n\n\n\n\n\n\nFigure 6\n \n \nSample of a MGF file\n. MGF files are used for enconding MS results, multiple spectra can be enconded in one MGF. Every spectrum begins with the \nBEGINS IONS\n assessment and finishes with \nEND IONS\n. MGF files can be divided in 2 parts : \n    \n The header : containing information about the embedded Search Parameters. \n    \n Ions information : the first figure is the ion mass, the second is the ion charge.\n\n\n\n\nX!Tandem\n\n\nX!Tandem\n is an open source software that can match tandem mass spectra (usually the experiment) with peptide sequences from a database. This process allows identification of proteins in one or more samples.\n\n\nThe X!Tandem search engine calculates a statistical confidence (expectation value) for all of the individual spectrum-to-sequence assignments. Some spectra might map to more than one protein, in this case X!Tandem will pick the best match with the highest confidence score (see \nFigure 7\n). The output is a lists all of the high confidence assignments.\n\n\n\n\n\n\nFigure 7\n \n \nSchema of the X!Tandem analysis\n[^figure1]. After LC-MS/MS analysis the experimental spectrum is compared to a theoretical spectrum made from a protein database. This comparison leads to a list of peptide match ranked by score.\n\n\n\n\nGALAXY\n\n\nGALAXY[^galaxy] is an open source, web-based platform for biomedical research. GALAXY allows users to perform multi-omics data analyses: genomics, proteomics, transcriptomics and more. Numerous tools are available to achieve various analyses. This tutorial will focus on proteomics identification: matching the experimental data, spectra from LC-MS/MS analysis against data from a protein database using X!Tandem.\n\n\nBefore starting, a quick overview of the GALAXY interface \n see \nFigure 8\n. The interface is divided into three parts:\n\n\n\n\nLeft panel\n:  List the tools that are available.  A search textbox is at the top of the panel in order to find the tool you want.\n\n\nRight panel\n: Is the history of all the data outputs from previous steps and subsequently becoming the input for the next step. For example, the figure below shows the data imported ready for the next step. This panel allows the user to follow the different steps of the  analysis.\n\n\nCentral panel\n: Is the main screen, showing the details and options of the selected tool.\n\n\n\n\n\n\n\n\nFigure 8\n \n \nGalaxy interface\n. Divided in 3 parts Galaxy\ns interface go from the left selecting the tools to the right where the results are displayed.\n\n\n\n\n\n\nTutorial\n\n\nThis tutorial describes how to identify a list of proteins from tandem mass spectrometry data.\n\n\nAnalyses of this type are a fundamental part of most proteomics studies. The basic idea is to match tandem MS spectra obtained from a sample with equivalent theoretical spectra against a reference protein database. The process is referred to as \nprotein identification\n, although amino acid sequences are not obtained \nde novo\n with this method.\n\n\nObjectives\n\n\nThe objective of this tutorial is to perform protein identification from MS/MS spectra data using the X!Tandem tool in GALAXY \n see \nFigure 9\n. The basic steps involved are:\n\n\n\n\nLoading UniProt[^uniprot] proteome data in GALAXY (fasta file format)\n\n\nLoading your MS/MS spectra in GALAXY\n\n\nRun X!Tandem proteomics search\n\n\nSorting and analysing the results\n\n\n\n\nThe tutorial will finish with an exercise where you repeat the same protocol \nbut\n with your own proteome as the reference database instead of using UniProt.\n\n\n\n\n\n\nFigure 9\n - \nGeneral flowchart of this training\n. The first part is to upload datasets to be investigated. Then the training will cover the key parts of the configuration of a X!Tandem search. Finally, we look at sorting and analysing the results.\n\n\n\n\nThe aim of the training will be to create a list of all proteins that can be confidently said to be present in the sample.\n\n\nThis tutorial uses the following open source tools:\n\n\n\n\nX!Tandem search engine\n\n\nTrans Proteomic Pipeline[^tpp]\n\n\nGALAXY platform with tools already installed\n\n\n\n\nThis tutorial uses an \nE. Coli\n MS/MS spectra dataset that can be downloaded from: \nEColi K12 Dataset\n\n\nOriginal source : \nhttp://www.marcottelab.org/MSdata/\n\n\nSTEP 1: Data import\n\n\n\n\nBefore importing data, \nName\n your history. \nClick\n on the \nUnnamed history\n on the top of the right panel until you get the cursor. \nDelete\n and \ntype\n in \nProtein Identificaiton E.coli K12\n or a more meaningful name. You \nmust\n \nhit Enter\n, otherwise the name will not be saved.\n\n\n\n\n\n\n\n\nNext, \nimport\n data into GALAXY. On the left panel \nclick\n on the upload button as shown below:\n\n\n\n\n\n\n\n\nA new window will open, where you can select a method to upload your data: Choose local file, Choose FTP file, Paste/Fetch data. \nClick\n on \nPaste/Fetch data\n then copy and paste the URL of the mass spectrometer file: into the textbox: EColi_K12_MS_Spectra.mgf\n\n\n\n\n\n\nTip\n : You can also use the \nGet Data \n Upload file\n tool to obtain the same result. Here you want to upload your MS/MS spectra.\n\n\n\n\n\n\n\n\nWarning\n : X!Tandem only accepts mgf files in GALAXY.  Other file formats have to be converted beforehand. A useful tool for that is msconvert\n^msconvert\n.\n\n\n\n\nSTEP 2: Import Reference Data\n\n\nWe will first use the UniProt Database as our reference data to search against.\n\n\n\n\nSelect the tool named \nProtein Database Downloader\n\n\nChoose the database: \nUniProtKB\n\n\nSelect the organism of interest: \nEscherichia Coli (strain K12)\n\n\n\n\nClick on \nExecute\n\n\n\n\n\n\nYou will see your history update with the new data imports\n\n\n\n\n\n\n\n\n\n\nRename your \nProtein Database\n by clicking on \n icon.\n\n\nSelect \nEdit Attributes\n\n\nIn \nName\n, type in \nEColi_K12_UniProt_Database\n\n\nClick \nSave\n\n\n\n\n\n\nSTEP 3: X!Tandem MS/MS Search\n\n\nThis part of the tutorial is to perform the X!Tandem MS/MS search. \n\n\n\n\nThe tool can be found in the left panel under the section \nProteomics Tools \n X!Tandem MSMS Search\n\n\nIn the central section, you should see the following options. Below the key parameters are explained in detail. \n\n\n\n\n\n\nX!Tandem proposes many options, the key options of interest are:\n\n\n\n\nUploaded FASTA file\n: this parameter is to select the fasta file that will be used as the proteins database.\n\n\nMSMS File\n : select the spectra file to analyse.\n\n\nVariable Modifications\n: this option considers possible modification on each residue (which impact the MS/MS spectra).\n\n\nFixed Modifications\n: this option allows you to specify any known modification.\n\n\nMissed Cleavages Allowed\n: \nwhen a cleavage reagent (either chemical or enzymatic) is used to cleave the peptide backbone of a protein to produce peptides, different sites along the backbone will have different reaction rates and kinetics. Therefore, there may be some sites that should be cleaved by the reagent that are not completely cleaved during the course of the experiment. The value of this parameter represents the maximum number of missed cleavage sites allowed within a peptide.\n\n\nEnzyme\n: specify the enzyme that has been used to cleave the proteins (affecting the interpretation of the spectrum).\n\n\nFragment ion tolerance\n: define the minimum weight (in Da) of the fragmented ions, default value is 0.5.\n\n\nPrecursor ion tolerance\n: define the minimum weight (in Da) of the precursor ions.\n\n\n\n\nIn this tutorial, we are using the following parameters:\n\n\n\n\n\n\n\n\nParameters Name\n\n\nValue\n\n\nDefault Value\n\n\n\n\n\n\n\n\n\n\nUploaded FASTA file\n\n\nEColi_K12_UniProt_Database\n\n\n\n\n\n\n\n\nMSMS File\n\n\nEColi_K12_MS_Spectra.mgf\n\n\n\n\n\n\n\n\nVariable Modifications\n\n\nOxidation M\n\n\n\n\n\n\n\n\nFixed Modifications\n\n\nCarbamidomethyl C\n\n\n\n\n\n\n\n\nMissed Cleavages Allowed\n\n\n2\n\n\n2\n\n\n\n\n\n\nEnzyme\n\n\nTrypsin\n\n\nTrypsin\n\n\n\n\n\n\nFragment ion tolerance\n\n\n0.5\n\n\n0.5\n\n\n\n\n\n\nPrecursor ion tolerance\n\n\n10 ppm\n\n\n10 ppm\n\n\n\n\n\n\n\n\n\n\nLeave all other parameters as their default settings.\n\n\n\n\nClick on \nExecute\n\n\n\n\n\n\nThe history should update with a new entry, the output file of the X!Tandem\n\n\n\n\nRename the output by clicking on the \n  icon\n\n\n\n\n\n\n\n\nYou can view the output by click on the name in the history panel.\n\n\n\n\nSTEP 4: Convert X!Tandem XML to Table\n\n\nThe output of X!Tandem is an XML format, which is not easy to decipher. In order to get a more readable file, we will convert the XML format to a table. This is a two step process:\n\n\n\n\nSelect \nProteomics Tools \n Tandem to pepXML\n\n\nSelect your \ntandem\n file in the \nInput File\n field\n\n\nClick on \nExecute\n\n\nThe history should update with a new \npepXML\n file. The pepXML file is still a XML file and needs to be converted to a tabular.\n\n\nSelect \nProteomics Tools \n PepXML to Table\n\n\nSelect your \npepXML\n file in the \nInput File\n field\n\n\nThis history should update with a new file\n\n\n\n\n\n\nAfter the X!Tandem search we obtain a list of proteins present in the sample data from Step 1:\n\n\n\n\n\n\n\n\nTabular name\n\n\nTandem file XML designation\n\n\nDefinition\n\n\n\n\n\n\n\n\n\n\nProtein\n\n\nlabel\n\n\nProtein name according to the database used for the MS/MS search\n\n\n\n\n\n\nPeptide\n\n\nseq\n\n\nPeptide sequence\n\n\n\n\n\n\nAssumed_charge\n\n\nz\n\n\nParent ion mass (plus a proton) from the spectrum\n\n\n\n\n\n\nCalc_neutral_pep_mass\n\n\nmh (+mass of a proton)\n\n\nParent ion mass calculated from the spectrum\n\n\n\n\n\n\nNeutral_mass\n\n\nmh (+mass of a proton)\n\n\nCalculated peptide mass (plus a proton)\n\n\n\n\n\n\nRetention_time\n\n\nrt\n\n\nLength of time between injection and position of the target compound peak.[^rt]\n\n\n\n\n\n\nStart_scan\n\n\nid\n\n\nid of the group treated (where the analysis starts)\n\n\n\n\n\n\nEnd_scan\n\n\nid\n\n\nid of the domain treated (usually the same as start_scan, no consideration of the doted name in the original XML file)\n\n\n\n\n\n\nSearch_engine\n\n\n\n\nName of the search engine used, in our case X!Tandem (associated with the scoring method : \nk-score\n)\n\n\n\n\n\n\nRaw_score\n\n\nexpect\n\n\nExpectation value for the top ranked protein identified with this spectrum\n\n\n\n\n\n\n\n\n\n\nNote:\n You can find all the details on the X!Tandem output file here: \nThe file format for X! series search engines\n. The tandem XML file contains more information than the tabular format which can also be of further use (e.i. the score for the different ions \nx, y, z, a, b, c\n \n)\n\n\n\n\nExercise\n\n\n\n\nRepeat the tutorial but instead of uploading a UniProt database in Step 2, upload your own database. You can use the \nE. Coli\n dataset : \nE. Coli Annotated Genome\n and compare the two outputs.\n\n\n\n\n\n\nReferences\n\n\n[^xtand]: X!Tandem website: http://www.thegpm.org/tandem/. X!Tandem documentation : http://www.thegpm.org/TANDEM/api/. Craig, R., and R. C. Beavis. 2004. \n\u201cTANDEM: matching proteins with tandem mass spectra.\u201d\n Bioinformatics 20, no. 9 (June): 1466-67. http://dx.doi.org/10.1093/bioinformatics/bth092.\n\n\n[^figure1]: Nesvizhskii, Alexey I. \nProtein Identification By Tandem Mass Spectrometry And Sequence Database Searching\n. Mass Spectrometry Data Analysis in Proteomics 87-120. \n \n \nMass Spectrometry Data Analysis in Proteomics\n\n\n[^galaxy]: GALAXY platform : https://usegalaxy.org/. You can also find a more extensive documentation here : https://galaxyproject.org/.\nAfgan, Enis et al. \nThe Galaxy Platform For Accessible, Reproducible And Collaborative Biomedical Analyses: 2016 Update\n. Nucleic Acids Res 44.W1 (2016): W3-W10.\n \n \nThe Galaxy platform for accessible, reproducible and collaborative biomedical analyses: 2016 update\n\n\n[^uniprot]: UniProt : http://www.uniprot.org/. Apweiler, R. \nUniprot: The Universal Protein Knowledgebase\n. Nucleic Acids Research 32.90001 (2004): 115D-119.\n \n \nUniProt: the Universal Protein knowledgebase\n\n\n[^figure3]: High-performance liquid chromatography (HPLC): \nHow Does High Performance Liquid Chromatography Work ?\n Mant, Colin T. et al. \nHPLC Analysis And Purification Of Peptides\n. Peptide Characterization and Application Protocols (2007): 3-55.\n\n\n[^figure4]: Aebersold, Ruedi and Matthias Mann. \nMass Spectrometry-Based Proteomics\n. Nature 422.6928 (2003): 198-207.\n \n \nMass Spectrometry-Based Proteomics\n\n\n[^MS file format]: Deutsch, E. W. \nFile Formats Commonly Used In Mass Spectrometry Proteomics\n. Molecular \n Cellular Proteomics 11.12 (2012): 1612-1621.\n \n \nFile Formats Commonly Used in Mass Spectrometry Proteomics\n\n\n[^MGF]: Mascot Generic Format (MGF) file is likely the most common text format for MS/MS data encoding in the form of a peak list. This file encodes multiple MS/MS spectra in a single file via m/z, intensity pairs separated by headers.\n \n \nMGF file format\n\n\n[^embedded_parameters]: The MGF format allows parameters that can be found after the BEGIN IONS statement.\n \n \nEmbedded Parameters\n\n\n[^rt]: The retention time of a peak can be used as means of qualitative identification. The length of time between injection and position of the target compound peak.\n \n \nRetention Time Parameters\n\n \n \nRetention Time explained for GC/MS\n\n\n[^tpp]: The Trans-Proteomic Pipeline (TPP) website : http://tools.proteomecenter.org/wiki/index.php?title=Software:TPP. Deutsch, Eric W. et al. \nA Guided Tour Of The Trans-Proteomic Pipeline\n. Proteomics 10.6 (2010): 1150-1159.\n \n \nA Guided Tour of the Trans-Proteomic Pipeline\n\n\n[^msconvert]: ProteoWizard Tools website : http://proteowizard.sourceforge.net/tools.shtml. Holman, Jerry D., David L. Tabb, and Parag Mallick. \nEmploying Proteowizard To Convert Raw Mass Spectrometry Data\n. Current Protocols in Bioinformatics (2014): 13.24.1-13.24.9.\n\n \nEmploying ProteoWizard to Convert Raw Mass Spectrometry Data", 
            "title": "Protein Identification"
        }, 
        {
            "location": "/modules/xtandem/#protein-indentification-using-xtandem", 
            "text": "", 
            "title": "Protein indentification using X!Tandem"
        }, 
        {
            "location": "/modules/xtandem/#introduction", 
            "text": "The high-throughput nature of proteomics mass spectrometry is enabled by a productive combination of data acquisition protocols and the computational tools used to interpret the resulting spectra (list of peaks). One of the key components in mainstream protocols is the generation of tandem mass (MS/MS) spectra by peptide fragmentation. This approach is currently used in the large majority of proteomics experiments to routinely identify hundreds to thousands of proteins from single mass spectrometry runs. In order to do that multiple tools have to be employed:  LC-MS/MS  to segregate components of proteomic samples associated with  protein identification  (see  Figure 1 ) softwares,  X!Tandem [^xtand],  Mascot  or  SEQUEST  all of which perform protein identification but with different algorithms.    Figure 1     General overview of the experimental steps and flow of data in protein identification using shotgun proteomics experiment. [^figure1].    Figure 1  shows a general overview of the experimental steps in protein identification. Sample proteins are first cleaved into peptides, which are then separated using chromatography (e.g. HPLC). The peptides are then ionised and then selected ions are fragmented to produce signature tandem mass spectrometry (MS/MS) spectra. Peptides are identified from the MS/MS spectra by using programs such as X!Tandem, which search against a database of known peptides. Sequences of the identified peptides are used to infer which proteins are present in the original sample.", 
            "title": "Introduction"
        }, 
        {
            "location": "/modules/xtandem/#background", 
            "text": "", 
            "title": "Background"
        }, 
        {
            "location": "/modules/xtandem/#lc-msms-analysis", 
            "text": "Liquid Chromatography  (LC) is a technique used to separate molecules in the solvent (mobile phase). Nowadays liquid chromatography utilising high pressure to force the solvent through the columns packed with porous particles (stationary phase) is referred as High Pressure Liquid Chromatography (HPLC)   see  Figure 2 . After purifying the proteomic cell content, LC is able to separate the different proteins which are injected in the mass spectrometer. Each peak from the results will be analysed by mass spectrometry.    Figure 2     Schema of High Pressure Liquid Chromatography (HPLC)[^figure3].  Compounds of the mixture are separated in the HPLC column according to various parameters (polarity, charge, affinity etc). The type of separation depends on the column being used). A detector flow cell is used to detect the separated compounds band.   Mass spectrometry  (MS)   see  Figure 3    has been widely used to analyse biological samples and has evolved into an indispensable tool for proteomics research. Fundamentally, MS measures the mass-to-charge ratio (m/z) of gas-phase ions. Mass spectrometers consist of an ion source that converts analyte molecules into gas-phase ions, a mass analyser that separates ionised analytes on the basis of m/z ratio and a detector that records the number of ions at each m/z value.     Figure 3     Schema of mass specter .  A mass spectrometer consists of three components: an ion source, a mass analyzer, and a detector. The ionizer converts a portion of the sample into ions. There is a wide variety of ionization techniques, depending on the phase (solid, liquid, gas) of the sample and the efficiency of various ionization mechanisms for the unknown species. An extraction system removes ions from the sample, which are then targeted through the mass analyzer and onto the detector. The differences in masses of the fragments allows the mass analyzer to sort the ions by their mass-to-charge ratio. The detector measures the value of an indicator quantity and thus provides data for calculating the abundances of each ion present.  [^figure4]   Tandem mass spectrometry  (MS/MS)   see  Figure 4    is a key technique for protein or peptide sequencing and post-translational modifications analysis. Collision-induced dissociation (CID) has been the most widely used MS/MS technique in proteomics research. In this method, gas-phase peptide/protein cations are internally heated by multiple collisions with rare gas atoms. The result is fragmented ions that, after the detection phase and reconstitution, reveal the amino-acid chains.     Figure 4     Schema of a tandem mass spectrometry associated with liquid chromatography (LC-MS/MS) . This technique combine separation form liquid chromatography and m/z ratio analyse of the tandem mass spectrometry in order to generate spectra.   File formats  : During a full proteomics analysis, as seen in  Figure 5 , many files are created. Every step has it s own multiples files formats :    Figure 5     Multiple formats during MS treatment [^MS file format]. From the sample processing to the final file, various file formats can be used. This schema shows the different processes and file formats that are generated during a full MS experiment.   For this tutorial we will focus on the  Informatics Analysis  part using the following file formats:   fasta : fasta files are mainly used for representing either nucleotide sequences or peptide sequences, using single-letter codes.  MGF  : Mascot Generic Format (MGF) file is the most common format for MS/MS data encoding in the form of a peak list[^MGF]. This file encodes multiple MS/MS spectra in a single file listing the [m/z, intensity] pairs separated by headers   see  Figure 6 . More precisely each query represents a complete MS/MS spectrum delimited by BEGIN IONS and END IONS statements. Embedded parameters[^embedded_parameters] can be found after each BEGIN IONS statement. An example entry is shown in the figure below:     Figure 6     Sample of a MGF file . MGF files are used for enconding MS results, multiple spectra can be enconded in one MGF. Every spectrum begins with the  BEGINS IONS  assessment and finishes with  END IONS . MGF files can be divided in 2 parts : \n      The header : containing information about the embedded Search Parameters. \n      Ions information : the first figure is the ion mass, the second is the ion charge.", 
            "title": "LC-MS/MS Analysis"
        }, 
        {
            "location": "/modules/xtandem/#xtandem", 
            "text": "X!Tandem  is an open source software that can match tandem mass spectra (usually the experiment) with peptide sequences from a database. This process allows identification of proteins in one or more samples.  The X!Tandem search engine calculates a statistical confidence (expectation value) for all of the individual spectrum-to-sequence assignments. Some spectra might map to more than one protein, in this case X!Tandem will pick the best match with the highest confidence score (see  Figure 7 ). The output is a lists all of the high confidence assignments.    Figure 7     Schema of the X!Tandem analysis [^figure1]. After LC-MS/MS analysis the experimental spectrum is compared to a theoretical spectrum made from a protein database. This comparison leads to a list of peptide match ranked by score.", 
            "title": "X!Tandem"
        }, 
        {
            "location": "/modules/xtandem/#galaxy", 
            "text": "GALAXY[^galaxy] is an open source, web-based platform for biomedical research. GALAXY allows users to perform multi-omics data analyses: genomics, proteomics, transcriptomics and more. Numerous tools are available to achieve various analyses. This tutorial will focus on proteomics identification: matching the experimental data, spectra from LC-MS/MS analysis against data from a protein database using X!Tandem.  Before starting, a quick overview of the GALAXY interface   see  Figure 8 . The interface is divided into three parts:   Left panel :  List the tools that are available.  A search textbox is at the top of the panel in order to find the tool you want.  Right panel : Is the history of all the data outputs from previous steps and subsequently becoming the input for the next step. For example, the figure below shows the data imported ready for the next step. This panel allows the user to follow the different steps of the  analysis.  Central panel : Is the main screen, showing the details and options of the selected tool.     Figure 8     Galaxy interface . Divided in 3 parts Galaxy s interface go from the left selecting the tools to the right where the results are displayed.", 
            "title": "GALAXY"
        }, 
        {
            "location": "/modules/xtandem/#tutorial", 
            "text": "This tutorial describes how to identify a list of proteins from tandem mass spectrometry data.  Analyses of this type are a fundamental part of most proteomics studies. The basic idea is to match tandem MS spectra obtained from a sample with equivalent theoretical spectra against a reference protein database. The process is referred to as  protein identification , although amino acid sequences are not obtained  de novo  with this method.", 
            "title": "Tutorial"
        }, 
        {
            "location": "/modules/xtandem/#objectives", 
            "text": "The objective of this tutorial is to perform protein identification from MS/MS spectra data using the X!Tandem tool in GALAXY   see  Figure 9 . The basic steps involved are:   Loading UniProt[^uniprot] proteome data in GALAXY (fasta file format)  Loading your MS/MS spectra in GALAXY  Run X!Tandem proteomics search  Sorting and analysing the results   The tutorial will finish with an exercise where you repeat the same protocol  but  with your own proteome as the reference database instead of using UniProt.    Figure 9  -  General flowchart of this training . The first part is to upload datasets to be investigated. Then the training will cover the key parts of the configuration of a X!Tandem search. Finally, we look at sorting and analysing the results.   The aim of the training will be to create a list of all proteins that can be confidently said to be present in the sample.  This tutorial uses the following open source tools:   X!Tandem search engine  Trans Proteomic Pipeline[^tpp]  GALAXY platform with tools already installed   This tutorial uses an  E. Coli  MS/MS spectra dataset that can be downloaded from:  EColi K12 Dataset  Original source :  http://www.marcottelab.org/MSdata/", 
            "title": "Objectives"
        }, 
        {
            "location": "/modules/xtandem/#step-1-data-import", 
            "text": "Before importing data,  Name  your history.  Click  on the  Unnamed history  on the top of the right panel until you get the cursor.  Delete  and  type  in  Protein Identificaiton E.coli K12  or a more meaningful name. You  must   hit Enter , otherwise the name will not be saved.     Next,  import  data into GALAXY. On the left panel  click  on the upload button as shown below:     A new window will open, where you can select a method to upload your data: Choose local file, Choose FTP file, Paste/Fetch data.  Click  on  Paste/Fetch data  then copy and paste the URL of the mass spectrometer file: into the textbox: EColi_K12_MS_Spectra.mgf    Tip  : You can also use the  Get Data   Upload file  tool to obtain the same result. Here you want to upload your MS/MS spectra.     Warning  : X!Tandem only accepts mgf files in GALAXY.  Other file formats have to be converted beforehand. A useful tool for that is msconvert ^msconvert .", 
            "title": "STEP 1: Data import"
        }, 
        {
            "location": "/modules/xtandem/#step-2-import-reference-data", 
            "text": "We will first use the UniProt Database as our reference data to search against.   Select the tool named  Protein Database Downloader  Choose the database:  UniProtKB  Select the organism of interest:  Escherichia Coli (strain K12)   Click on  Execute    You will see your history update with the new data imports      Rename your  Protein Database  by clicking on   icon.  Select  Edit Attributes  In  Name , type in  EColi_K12_UniProt_Database  Click  Save", 
            "title": "STEP 2: Import Reference Data"
        }, 
        {
            "location": "/modules/xtandem/#step-3-xtandem-msms-search", 
            "text": "This part of the tutorial is to perform the X!Tandem MS/MS search.    The tool can be found in the left panel under the section  Proteomics Tools   X!Tandem MSMS Search  In the central section, you should see the following options. Below the key parameters are explained in detail.     X!Tandem proposes many options, the key options of interest are:   Uploaded FASTA file : this parameter is to select the fasta file that will be used as the proteins database.  MSMS File  : select the spectra file to analyse.  Variable Modifications : this option considers possible modification on each residue (which impact the MS/MS spectra).  Fixed Modifications : this option allows you to specify any known modification.  Missed Cleavages Allowed :  when a cleavage reagent (either chemical or enzymatic) is used to cleave the peptide backbone of a protein to produce peptides, different sites along the backbone will have different reaction rates and kinetics. Therefore, there may be some sites that should be cleaved by the reagent that are not completely cleaved during the course of the experiment. The value of this parameter represents the maximum number of missed cleavage sites allowed within a peptide.  Enzyme : specify the enzyme that has been used to cleave the proteins (affecting the interpretation of the spectrum).  Fragment ion tolerance : define the minimum weight (in Da) of the fragmented ions, default value is 0.5.  Precursor ion tolerance : define the minimum weight (in Da) of the precursor ions.   In this tutorial, we are using the following parameters:     Parameters Name  Value  Default Value      Uploaded FASTA file  EColi_K12_UniProt_Database     MSMS File  EColi_K12_MS_Spectra.mgf     Variable Modifications  Oxidation M     Fixed Modifications  Carbamidomethyl C     Missed Cleavages Allowed  2  2    Enzyme  Trypsin  Trypsin    Fragment ion tolerance  0.5  0.5    Precursor ion tolerance  10 ppm  10 ppm      Leave all other parameters as their default settings.   Click on  Execute    The history should update with a new entry, the output file of the X!Tandem   Rename the output by clicking on the    icon     You can view the output by click on the name in the history panel.", 
            "title": "STEP 3: X!Tandem MS/MS Search"
        }, 
        {
            "location": "/modules/xtandem/#step-4-convert-xtandem-xml-to-table", 
            "text": "The output of X!Tandem is an XML format, which is not easy to decipher. In order to get a more readable file, we will convert the XML format to a table. This is a two step process:   Select  Proteomics Tools   Tandem to pepXML  Select your  tandem  file in the  Input File  field  Click on  Execute  The history should update with a new  pepXML  file. The pepXML file is still a XML file and needs to be converted to a tabular.  Select  Proteomics Tools   PepXML to Table  Select your  pepXML  file in the  Input File  field  This history should update with a new file    After the X!Tandem search we obtain a list of proteins present in the sample data from Step 1:     Tabular name  Tandem file XML designation  Definition      Protein  label  Protein name according to the database used for the MS/MS search    Peptide  seq  Peptide sequence    Assumed_charge  z  Parent ion mass (plus a proton) from the spectrum    Calc_neutral_pep_mass  mh (+mass of a proton)  Parent ion mass calculated from the spectrum    Neutral_mass  mh (+mass of a proton)  Calculated peptide mass (plus a proton)    Retention_time  rt  Length of time between injection and position of the target compound peak.[^rt]    Start_scan  id  id of the group treated (where the analysis starts)    End_scan  id  id of the domain treated (usually the same as start_scan, no consideration of the doted name in the original XML file)    Search_engine   Name of the search engine used, in our case X!Tandem (associated with the scoring method :  k-score )    Raw_score  expect  Expectation value for the top ranked protein identified with this spectrum      Note:  You can find all the details on the X!Tandem output file here:  The file format for X! series search engines . The tandem XML file contains more information than the tabular format which can also be of further use (e.i. the score for the different ions  x, y, z, a, b, c   )", 
            "title": "STEP 4: Convert X!Tandem XML to Table"
        }, 
        {
            "location": "/modules/xtandem/#exercise", 
            "text": "Repeat the tutorial but instead of uploading a UniProt database in Step 2, upload your own database. You can use the  E. Coli  dataset :  E. Coli Annotated Genome  and compare the two outputs.", 
            "title": "Exercise"
        }, 
        {
            "location": "/modules/xtandem/#references", 
            "text": "[^xtand]: X!Tandem website: http://www.thegpm.org/tandem/. X!Tandem documentation : http://www.thegpm.org/TANDEM/api/. Craig, R., and R. C. Beavis. 2004.  \u201cTANDEM: matching proteins with tandem mass spectra.\u201d  Bioinformatics 20, no. 9 (June): 1466-67. http://dx.doi.org/10.1093/bioinformatics/bth092.  [^figure1]: Nesvizhskii, Alexey I.  Protein Identification By Tandem Mass Spectrometry And Sequence Database Searching . Mass Spectrometry Data Analysis in Proteomics 87-120. \n    Mass Spectrometry Data Analysis in Proteomics  [^galaxy]: GALAXY platform : https://usegalaxy.org/. You can also find a more extensive documentation here : https://galaxyproject.org/.\nAfgan, Enis et al.  The Galaxy Platform For Accessible, Reproducible And Collaborative Biomedical Analyses: 2016 Update . Nucleic Acids Res 44.W1 (2016): W3-W10.\n    The Galaxy platform for accessible, reproducible and collaborative biomedical analyses: 2016 update  [^uniprot]: UniProt : http://www.uniprot.org/. Apweiler, R.  Uniprot: The Universal Protein Knowledgebase . Nucleic Acids Research 32.90001 (2004): 115D-119.\n    UniProt: the Universal Protein knowledgebase  [^figure3]: High-performance liquid chromatography (HPLC):  How Does High Performance Liquid Chromatography Work ?  Mant, Colin T. et al.  HPLC Analysis And Purification Of Peptides . Peptide Characterization and Application Protocols (2007): 3-55.  [^figure4]: Aebersold, Ruedi and Matthias Mann.  Mass Spectrometry-Based Proteomics . Nature 422.6928 (2003): 198-207.\n    Mass Spectrometry-Based Proteomics  [^MS file format]: Deutsch, E. W.  File Formats Commonly Used In Mass Spectrometry Proteomics . Molecular   Cellular Proteomics 11.12 (2012): 1612-1621.\n    File Formats Commonly Used in Mass Spectrometry Proteomics  [^MGF]: Mascot Generic Format (MGF) file is likely the most common text format for MS/MS data encoding in the form of a peak list. This file encodes multiple MS/MS spectra in a single file via m/z, intensity pairs separated by headers.\n    MGF file format  [^embedded_parameters]: The MGF format allows parameters that can be found after the BEGIN IONS statement.\n    Embedded Parameters  [^rt]: The retention time of a peak can be used as means of qualitative identification. The length of time between injection and position of the target compound peak.\n    Retention Time Parameters \n    Retention Time explained for GC/MS  [^tpp]: The Trans-Proteomic Pipeline (TPP) website : http://tools.proteomecenter.org/wiki/index.php?title=Software:TPP. Deutsch, Eric W. et al.  A Guided Tour Of The Trans-Proteomic Pipeline . Proteomics 10.6 (2010): 1150-1159.\n    A Guided Tour of the Trans-Proteomic Pipeline  [^msconvert]: ProteoWizard Tools website : http://proteowizard.sourceforge.net/tools.shtml. Holman, Jerry D., David L. Tabb, and Parag Mallick.  Employing Proteowizard To Convert Raw Mass Spectrometry Data . Current Protocols in Bioinformatics (2014): 13.24.1-13.24.9.   Employing ProteoWizard to Convert Raw Mass Spectrometry Data", 
            "title": "References"
        }, 
        {
            "location": "/about/", 
            "text": "About\n\n\nThe Food and Health Flagship is an RDS-funded project to provide cloud-based data services and tools\nfor Australian Life Science Researchers to combine, analyse and interpret\ngenomic, transcriptomic, proteomic and metabolomic data. The data platform will incorporate the Bioplatforms Australia Antibiotic Resistant Pathogens Initiative (ABRPI).\n\n\nAuthors\n\n\n Anna Syme\n\n\n Torsten Seemann\n\n\n Simon Gladman\n\n\n Dieter Bulach\n\n\n Xin-Yi Chua\n\n\n Dominique Gorse\n\n\nSupport\n\n\n\n\nResearch Data Services\n\n\nBioplatforms Australia\n\n\nNectar\n\n\n\n\n\n\nThese training materials have been used for:\n\n\nMcGill Summer Institute in Infectious Diseases and Global Health, June 2016, Montreal, Canada\n\n\n\n\n\n\nGalaxy Community Conference 2016, Indiana, USA\n\n\n\n\n\n\nCLIMB UK Launch: Cloud Infrastructure for Microbial Bioinformatics, 2016", 
            "title": "About"
        }, 
        {
            "location": "/about/#about", 
            "text": "The Food and Health Flagship is an RDS-funded project to provide cloud-based data services and tools\nfor Australian Life Science Researchers to combine, analyse and interpret\ngenomic, transcriptomic, proteomic and metabolomic data. The data platform will incorporate the Bioplatforms Australia Antibiotic Resistant Pathogens Initiative (ABRPI).", 
            "title": "About"
        }, 
        {
            "location": "/about/#authors", 
            "text": "Anna Syme   Torsten Seemann   Simon Gladman   Dieter Bulach   Xin-Yi Chua   Dominique Gorse", 
            "title": "Authors"
        }, 
        {
            "location": "/about/#support", 
            "text": "Research Data Services  Bioplatforms Australia  Nectar    These training materials have been used for:  McGill Summer Institute in Infectious Diseases and Global Health, June 2016, Montreal, Canada    Galaxy Community Conference 2016, Indiana, USA    CLIMB UK Launch: Cloud Infrastructure for Microbial Bioinformatics, 2016", 
            "title": "Support"
        }
    ]
}